{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U5_Actividad con PySpark\n",
    "\n",
    "TECNM Campus La Laguna\n",
    "\n",
    "Big Data\n",
    "\n",
    "**Alumno: 18131209 - ADAME SANDOVAL JOSE MISAEL**\n",
    "\n",
    "# ¿Qué es Spark?\n",
    "\n",
    "Apache Spark es una tecnología de cómputo de clústeres excepcional, diseñada para cálculos rápidos. Depende de Hadoop MapReduce y extiende el modelo de MapReduce para utilizarlo de manera efectiva para más tipos de cálculos, que incorporan preguntas intuitivas y manejo de flujos. El elemento fundamental de Spark es su agrupamiento en memoria que expande el ritmo de preparación de una aplicación. Spark puede procesar cantidades de datos en el orden de terabytes incluso petabytes.\n",
    "\n",
    "Spark utiliza Hadoop de dos maneras diferentes: una es para almacenamiento y la segunda para el manejo de procesos. Solo porque Spark tiene su propia administración de clústeres, utiliza Hadoop para el objetivo de almacenamiento.\n",
    "\n",
    "Spark está diseñado para cubrir una amplia variedad de cargas restantes, por ejemplo, aplicaciones de clústeres, cálculos iterativos, preguntas intuitivas y transmisión. Además de soportar todas estas tareas restantes en un marco particular, disminuye el peso de la administración de mantener aparatos aislados.\n",
    "\n",
    "## ¿Qué es PySpark?\n",
    "\n",
    "PySpark es una interfaz para Apache Spark en Python. No sólo permite escribir aplicaciones Spark utilizando las APIs de Python, sino que también proporciona el shell PySpark para analizar interactivamente sus datos en un entorno distribuido. PySpark soporta la mayoría de las características de Spark como Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) y Spark Core.\n",
    "\n",
    "## ¿Para qué es Spark?\n",
    "\n",
    "Apache Spark es un sistema de computación en clúster muy veloz. Proporciona el conjunto de API de alto nivel, a saber, Java, Scala, Python y R para el desarrollo de aplicaciones. Apache Spark es una herramienta para ejecutar rápidamente aplicaciones Spark.\n",
    "\n",
    "## Características\n",
    "\n",
    "* Está integrado con Apache Hadoop.\n",
    "* Trabaja en memoria, con lo que se consigue mucha mayor velocidad de procesamiento .\n",
    "* También permite trabajar en disco. De esta manera si por ejemplo tenemos un fichero muy grande o una cantidad de información que no cabe en memoria, la herramienta permite almacenar parte en disco, lo que hace perder velocidad. Esto hace que tengamos que intentar encontrar el equilibrio entre lo que se almacena en memoria y lo que se almacena en disco, para tener una buena velocidad y para que el coste no sea demasiado elevado, ya que la memoria siempre es bastante más cara que el disco.\n",
    "* Nos proporciona API para Java, Scala, Python y R.\n",
    "* Permite el procesamiento en tiempo real, con un módulo llamado Spark Streaming, que combinado con Spark SQL nos va a permitir el procesamiento en tiempo real de los datos. Conforme vayamos inyectando los datos podemos ir transformándolos y volcándolos a un resultado final.\n",
    "* Resilient Distributed Dataset (RDD): Usa la evaluación perezosa, lo que significa es que todas las transformaciones que vamos realizando sobre los RDD, no se resuelven, si no que se van almacenando en un grafo acíclico dirigido (DAG) , y cuando ejecutamos una acción, es decir, cuando la herramienta no tenga más opción que ejecutar todas las transformaciones, será cuando se ejecuten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # SparkSession\n",
    " \n",
    "SparkSession introducido en la versión 2.0, es un punto de entrada a la funcionalidad subyacente de PySpark con el fin de crear programáticamente PySpark RDD, DataFrame. Su objeto spark está disponible por defecto en pyspark-shell y puede ser creado programáticamente usando SparkSession.\n",
    "\n",
    "### Funcionalidad y características\n",
    "\n",
    "SparkSession es una clase combinada para todos los diferentes contextos que teníamos antes de la versión 2.0 (SQLContext y HiveContext, etc.). Desde la versión 2.0, SparkSession puede utilizarse en sustitución de SQLContext, HiveContext y otros contextos definidos antes de la versión 2.0.\n",
    "\n",
    "Como se mencionó al principio SparkSession es un punto de entrada a PySpark y la creación de una instancia de SparkSession sería la primera declaración que escribirías para programar con RDD, DataFrame y Dataset. La SparkSession se creará utilizando los patrones del constructor SparkSession.builder.\n",
    "\n",
    "Aunque SparkContext solía ser un punto de entrada antes de la versión 2.0, no se ha sustituido completamente por SparkSession, muchas características de SparkContext siguen estando disponibles y se utilizan en Spark 2.0 y posteriores. También debes saber que SparkSession crea internamente SparkConfig y SparkContext con la configuración proporcionada con SparkSession.\n",
    "\n",
    "Puedes crear tantos objetos SparkSession como quieras utilizando SparkSession.builder o SparkSession.newSession.\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Pruebita') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**master()** - Si se está ejecutando en el clúster es necesario utilizar el nombre de su maestro como un argumento a master(). por lo general, sería ya sea yarn o mesos depende de la configuración de su clúster.\n",
    "\n",
    "Utilice **local[x]** cuando se ejecuta en modo Standalone. x debe ser un valor entero y debe ser mayor que 0; esto representa cuántas particiones debe crear cuando se utiliza RDD, DataFrame, y Dataset. Idealmente, el valor de x debería ser el número de núcleos de la CPU que tiene.\n",
    "\n",
    "**appName()** - Se utiliza para establecer el nombre de su aplicación.\n",
    "\n",
    "**getOrCreate()** - Devuelve un objeto SparkSession si ya existe, crea uno nuevo si no existe.\n",
    "\n",
    "Nota: El objeto SparkSession \"spark\" está disponible por defecto en el shell de PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version - Devuelve la versión de Spark en la que se está ejecutando tu aplicación, probablemente la versión de Spark con la \n",
    "#que está configurado tu cluster\n",
    "\n",
    "spark.version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-JD7LALA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pruebita</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1659403bf10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getActiveSession() - devuelve una sesión activa de Spark.\n",
    "\n",
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.sql of <pyspark.sql.session.SparkSession object at 0x000001659403BF10>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sql - Devuelve un DataFrame después de ejecutar el SQL mencionado.\n",
    "\n",
    "spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop() - Detener el SparkContext actual.\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkContext\n",
    "\n",
    "Punto de entrada principal para la funcionalidad de Spark. Un SparkContext representa la conexión a un clúster de Spark, y puede utilizarse para crear RDDs, acumuladores y variables de difusión en ese clúster.\n",
    "\n",
    "### Funcionalidad y características\n",
    "\n",
    "SparkContext utiliza Py4J para lanzar una JVM y crea un JavaSparkContext. Por defecto, PySpark tiene SparkContext disponible como 'sc', por lo que crear un nuevo SparkContext no funcionará.\n",
    "\n",
    "Los siguientes son los parámetros de un SparkContext.\n",
    "\n",
    "* Master - Es la URL del cluster al que se conecta.\n",
    "* appName - Nombre de su trabajo.\n",
    "* sparkHome - Directorio de instalación de Spark.\n",
    "* pyFiles - Los archivos .zip o .py a enviar al cluster y añadir al PYTHONPATH.\n",
    "* Environment - Variables de entorno de los nodos de trabajo.\n",
    "* batchSize - El número de objetos Python representados como un único objeto Java. Establezca 1 para deshabilitar el batching, 0 para elegir automáticamente el tamaño del batch basado en el tamaño de los objetos, o -1 para utilizar un tamaño de batch ilimitado.\n",
    "* Serializador - Serializador RDD.\n",
    "* Conf - Un objeto de L{SparkConf} para establecer todas las propiedades de Spark.\n",
    "* Gateway - Utilizar una puerta de enlace y JVM existente, de lo contrario inicializar una nueva JVM.\n",
    "* JSC - La instancia de JavaSparkContext.\n",
    "* profiler_cls - Una clase de Profiler personalizada utilizada para hacer el profiling (el valor por defecto es pyspark.profiler.BasicProfiler).\n",
    "\n",
    "Entre los parámetros anteriores, master y appname son los más utilizados. \n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Tarea_Prueba\") # sc objeto que apunta a SC al cluster local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1623868247089'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applicationId - Devuelve un ID único de una aplicación Spark\n",
    "\n",
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[1]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#master - Devuelve el master que se estableció al crear SparkContext\n",
    "\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tarea_Prueba'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#appName - Devuelve el nombre de la aplicación que se dio al crear SparkContext\n",
    "\n",
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.broadcast of <SparkContext master=local[1] appName=Tarea_Prueba>>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#broadcast - difusión de la variable de sólo lectura a todo el clúster. Puedes difundir una variable a un clúster de \n",
    "#Spark sólo una vez.\n",
    "\n",
    "sc.broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.getOrCreate of <class 'pyspark.context.SparkContext'>>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getOrCreate - Crea o devuelve un SparkContext\n",
    "\n",
    "sc.getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
