{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U5_Actividad con PySpark\n",
    "\n",
    "TECNM Campus La Laguna\n",
    "\n",
    "Big Data\n",
    "\n",
    "**Alumno: 18131209 - ADAME SANDOVAL JOSE MISAEL**\n",
    "\n",
    "# ¿Qué es Spark?\n",
    "\n",
    "Apache Spark es una tecnología de cómputo de clústeres excepcional, diseñada para cálculos rápidos. Depende de Hadoop MapReduce y extiende el modelo de MapReduce para utilizarlo de manera efectiva para más tipos de cálculos, que incorporan preguntas intuitivas y manejo de flujos. El elemento fundamental de Spark es su agrupamiento en memoria que expande el ritmo de preparación de una aplicación. Spark puede procesar cantidades de datos en el orden de terabytes incluso petabytes.\n",
    "\n",
    "Spark utiliza Hadoop de dos maneras diferentes: una es para almacenamiento y la segunda para el manejo de procesos. Solo porque Spark tiene su propia administración de clústeres, utiliza Hadoop para el objetivo de almacenamiento.\n",
    "\n",
    "Spark está diseñado para cubrir una amplia variedad de cargas restantes, por ejemplo, aplicaciones de clústeres, cálculos iterativos, preguntas intuitivas y transmisión. Además de soportar todas estas tareas restantes en un marco particular, disminuye el peso de la administración de mantener aparatos aislados.\n",
    "\n",
    "## ¿Qué es PySpark?\n",
    "\n",
    "PySpark es una interfaz para Apache Spark en Python. No sólo permite escribir aplicaciones Spark utilizando las APIs de Python, sino que también proporciona el shell PySpark para analizar interactivamente sus datos en un entorno distribuido. PySpark soporta la mayoría de las características de Spark como Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) y Spark Core.\n",
    "\n",
    "## ¿Para qué es Spark?\n",
    "\n",
    "Apache Spark es un sistema de computación en clúster muy veloz. Proporciona el conjunto de API de alto nivel, a saber, Java, Scala, Python y R para el desarrollo de aplicaciones. Apache Spark es una herramienta para ejecutar rápidamente aplicaciones Spark.\n",
    "\n",
    "## Características\n",
    "\n",
    "* Está integrado con Apache Hadoop.\n",
    "* Trabaja en memoria, con lo que se consigue mucha mayor velocidad de procesamiento .\n",
    "* También permite trabajar en disco. De esta manera si por ejemplo tenemos un fichero muy grande o una cantidad de información que no cabe en memoria, la herramienta permite almacenar parte en disco, lo que hace perder velocidad. Esto hace que tengamos que intentar encontrar el equilibrio entre lo que se almacena en memoria y lo que se almacena en disco, para tener una buena velocidad y para que el coste no sea demasiado elevado, ya que la memoria siempre es bastante más cara que el disco.\n",
    "* Nos proporciona API para Java, Scala, Python y R.\n",
    "* Permite el procesamiento en tiempo real, con un módulo llamado Spark Streaming, que combinado con Spark SQL nos va a permitir el procesamiento en tiempo real de los datos. Conforme vayamos inyectando los datos podemos ir transformándolos y volcándolos a un resultado final.\n",
    "* **Resilient Distributed Dataset (RDD):** Usa la evaluación perezosa, lo que significa es que todas las transformaciones que vamos realizando sobre los RDD, no se resuelven, si no que se van almacenando en un grafo acíclico dirigido (DAG) , y cuando ejecutamos una acción, es decir, cuando la herramienta no tenga más opción que ejecutar todas las transformaciones, será cuando se ejecuten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # SparkSession\n",
    " \n",
    "SparkSession introducido en la versión 2.0, es un punto de entrada a la funcionalidad subyacente de PySpark con el fin de crear programáticamente PySpark RDD, DataFrame. Su objeto spark está disponible por defecto en pyspark-shell y puede ser creado programáticamente usando SparkSession.\n",
    "\n",
    "### Funcionalidad y características\n",
    "\n",
    "SparkSession es una clase combinada para todos los diferentes contextos que teníamos antes de la versión 2.0 (SQLContext y HiveContext, etc.). Desde la versión 2.0, SparkSession puede utilizarse en sustitución de SQLContext, HiveContext y otros contextos definidos antes de la versión 2.0.\n",
    "\n",
    "Como se mencionó al principio SparkSession es un punto de entrada a PySpark y la creación de una instancia de SparkSession sería la primera declaración que escribirías para programar con RDD, DataFrame y Dataset. La SparkSession se creará utilizando los patrones del constructor SparkSession.builder.\n",
    "\n",
    "Aunque SparkContext solía ser un punto de entrada antes de la versión 2.0, no se ha sustituido completamente por SparkSession, muchas características de SparkContext siguen estando disponibles y se utilizan en Spark 2.0 y posteriores. También debes saber que SparkSession crea internamente SparkConfig y SparkContext con la configuración proporcionada con SparkSession.\n",
    "\n",
    "Puedes crear tantos objetos SparkSession como quieras utilizando SparkSession.builder o SparkSession.newSession.\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Pruebita') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**master()** - Si se está ejecutando en el clúster es necesario utilizar el nombre de su maestro como un argumento a master(). por lo general, sería ya sea yarn o mesos depende de la configuración de su clúster.\n",
    "\n",
    "Utilice **local[x]** cuando se ejecuta en modo Standalone. x debe ser un valor entero y debe ser mayor que 0; esto representa cuántas particiones debe crear cuando se utiliza RDD, DataFrame, y Dataset. Idealmente, el valor de x debería ser el número de núcleos de la CPU que tiene.\n",
    "\n",
    "**appName()** - Se utiliza para establecer el nombre de su aplicación.\n",
    "\n",
    "**getOrCreate()** - Devuelve un objeto SparkSession si ya existe, crea uno nuevo si no existe.\n",
    "\n",
    "Nota: El objeto SparkSession \"spark\" está disponible por defecto en el shell de PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version - Devuelve la versión de Spark en la que se está ejecutando tu aplicación, probablemente la versión de Spark con la \n",
    "#que está configurado tu cluster\n",
    "spark.version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-JD7LALA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pruebita</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23766a54f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getActiveSession() - devuelve una sesión activa de Spark.\n",
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.sql of <pyspark.sql.session.SparkSession object at 0x0000023766A54F10>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sql - Devuelve un DataFrame después de ejecutar el SQL mencionado.\n",
    "spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop() - Detener el SparkContext actual.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkContext\n",
    "\n",
    "Punto de entrada principal para la funcionalidad de Spark. Un SparkContext representa la conexión a un clúster de Spark, y puede utilizarse para crear RDDs, acumuladores y variables de difusión en ese clúster.\n",
    "\n",
    "### Funcionalidad y características\n",
    "\n",
    "SparkContext utiliza Py4J para lanzar una JVM y crea un JavaSparkContext. Por defecto, PySpark tiene SparkContext disponible como 'sc', por lo que crear un nuevo SparkContext no funcionará.\n",
    "\n",
    "Los siguientes son los parámetros de un SparkContext.\n",
    "\n",
    "* Master - Es la URL del cluster al que se conecta.\n",
    "* appName - Nombre de su trabajo.\n",
    "* sparkHome - Directorio de instalación de Spark.\n",
    "* pyFiles - Los archivos .zip o .py a enviar al cluster y añadir al PYTHONPATH.\n",
    "* Environment - Variables de entorno de los nodos de trabajo.\n",
    "* batchSize - El número de objetos Python representados como un único objeto Java. Establezca 1 para deshabilitar el batching, 0 para elegir automáticamente el tamaño del batch basado en el tamaño de los objetos, o -1 para utilizar un tamaño de batch ilimitado.\n",
    "* Serializador - Serializador RDD.\n",
    "* Conf - Un objeto de L{SparkConf} para establecer todas las propiedades de Spark.\n",
    "* Gateway - Utilizar una puerta de enlace y JVM existente, de lo contrario inicializar una nueva JVM.\n",
    "* JSC - La instancia de JavaSparkContext.\n",
    "* profiler_cls - Una clase de Profiler personalizada utilizada para hacer el profiling (el valor por defecto es pyspark.profiler.BasicProfiler).\n",
    "\n",
    "Entre los parámetros anteriores, master y appname son los más utilizados. \n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Tarea_Prueba\") # sc objeto que apunta a SC al cluster local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1623998704925'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applicationId - Devuelve un ID único de una aplicación Spark\n",
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[1]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#master - Devuelve el master que se estableció al crear SparkContext\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tarea_Prueba'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#appName - Devuelve el nombre de la aplicación que se dio al crear SparkContext\n",
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.getOrCreate of <class 'pyspark.context.SparkContext'>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getOrCreate - Crea o devuelve un SparkContext\n",
    "sc.getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelize\n",
    "\n",
    "PySpark parallelize() es una función en SparkContext y se utiliza para crear un RDD a partir de una colección de listas. Pero ¿Qué es un RDD?\n",
    "\n",
    "**Resilient Distributed Datasets (RDD)** es una estructura de datos fundamental de PySpark, es una colección distribuida inmutable de objetos. Cada conjunto de datos en RDD se divide en particiones lógicas, que pueden ser calculadas en diferentes nodos del clúster.\n",
    "\n",
    "* PySpark Parallelizing es una colección existente en el driver del programa.\n",
    "\n",
    "La función parallelize() también tiene otra firma que adicionalmente toma un argumento entero para especificar el número de particiones. Las particiones son unidades básicas de paralelismo en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creando un RDD de 7 enteros: se crea en la memoria del proceso driver\n",
    "r = sc.parallelize([1,2,3,4,5,6,7])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creando un RDD de 4 cadenas de texto\n",
    "r = sc.parallelize([\"Hola\", \"Maestra\", \"Lamia\", \"<3\"])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de archivos TXT\n",
    "\n",
    "Cuando se quiere cargar un archivo plano a Spark se puede usar el método textFile(). Dicho método, nos devuelve un objeto de tipo <class ‘pyspark.rdd.RDD’>. El método sparkContext.textFile() se utiliza para leer un archivo de texto desde HDFS, S3 y cualquier sistema de archivos soportado por Hadoop, este método toma la ruta como argumento y opcionalmente toma un número de particiones como segundo argumento.\n",
    "\n",
    "### Ejemplo de lectura\n",
    "\n",
    "Es un .txt de un Web Scrapping sobre Romeo y Julieta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmas_\\Jupyter\\Datasets\\ryj.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "#usar el método textFile() para cargarlo a un RDD.\n",
    "rdd = sc.textFile(\"C:\\\\Users\\\\jmas_\\\\Jupyter\\\\Datasets\\\\ryj.txt\")\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare',\n",
       " '',\n",
       " '',\n",
       " '*******************************************************************',\n",
       " \"THIS EBOOK WAS ONE OF PROJECT GUTENBERG'S EARLY FILES PRODUCED AT A\",\n",
       " 'TIME WHEN PROOFING METHODS AND TOOLS WERE NOT WELL DEVELOPED. THERE',\n",
       " 'IS AN IMPROVED EDITION OF THIS TITLE WHICH MAY BE VIEWED AS EBOOK',\n",
       " '(#1513) at https://www.gutenberg.org/ebooks/1513',\n",
       " '*******************************************************************',\n",
       " '',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included',\n",
       " 'with this eBook or online at www.gutenberg.org/license',\n",
       " '',\n",
       " '',\n",
       " 'Title: Romeo and Juliet',\n",
       " '',\n",
       " 'Author: William Shakespeare',\n",
       " '',\n",
       " 'Posting Date: May 25, 2012 [EBook #1112]',\n",
       " 'Release Date: November, 1997  [Etext #1112]',\n",
       " '',\n",
       " 'Language: English',\n",
       " '',\n",
       " '',\n",
       " '*** START OF THIS PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '*Project Gutenberg is proud to cooperate with The World Library*',\n",
       " 'in the presentation of The Complete Works of William Shakespeare',\n",
       " 'for your reading for education and entertainment.  HOWEVER, THIS',\n",
       " 'IS NEITHER SHAREWARE NOR PUBLIC DOMAIN. . .AND UNDER THE LIBRARY',\n",
       " 'OF THE FUTURE CONDITIONS OF THIS PRESENTATION. . .NO CHARGES MAY',\n",
       " 'BE MADE FOR *ANY* ACCESS TO THIS MATERIAL.  YOU ARE ENCOURAGED!!',\n",
       " 'TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOWED!!',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'The Complete Works of William Shakespeare',\n",
       " '',\n",
       " 'The Tragedy of Romeo and Juliet',\n",
       " '',\n",
       " 'The Library of the Future Complete Works of William Shakespeare',\n",
       " 'Library of the Future is a TradeMark (TM) of World Library Inc.',\n",
       " '',\n",
       " '',\n",
       " '<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM',\n",
       " 'SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS',\n",
       " 'PROVIDED BY PROJECT GUTENBERG ETEXT OF CARNEGIE MELLON UNIVERSITY',\n",
       " 'WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE',\n",
       " 'DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS',\n",
       " 'PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED',\n",
       " 'COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY',\n",
       " 'SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '1595',\n",
       " '',\n",
       " 'THE TRAGEDY OF ROMEO AND JULIET',\n",
       " '',\n",
       " 'by William Shakespeare',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Dramatis Personae',\n",
       " '',\n",
       " '  Chorus.',\n",
       " '',\n",
       " '',\n",
       " '  Escalus, Prince of Verona.',\n",
       " '',\n",
       " '  Paris, a young Count, kinsman to the Prince.',\n",
       " '',\n",
       " '  Montague, heads of two houses at variance with each other.',\n",
       " '',\n",
       " '  Capulet, heads of two houses at variance with each other.',\n",
       " '',\n",
       " '  An old Man, of the Capulet family.',\n",
       " '',\n",
       " '  Romeo, son to Montague.',\n",
       " '',\n",
       " '  Tybalt, nephew to Lady Capulet.',\n",
       " '',\n",
       " '  Mercutio, kinsman to the Prince and friend to Romeo.',\n",
       " '',\n",
       " '  Benvolio, nephew to Montague, and friend to Romeo',\n",
       " '',\n",
       " '  Tybalt, nephew to Lady Capulet.',\n",
       " '',\n",
       " '  Friar Laurence, Franciscan.',\n",
       " '',\n",
       " '  Friar John, Franciscan.',\n",
       " '',\n",
       " '  Balthasar, servant to Romeo.',\n",
       " '',\n",
       " '  Abram, servant to Montague.',\n",
       " '',\n",
       " '  Sampson, servant to Capulet.',\n",
       " '',\n",
       " '  Gregory, servant to Capulet.',\n",
       " '',\n",
       " \"  Peter, servant to Juliet's nurse.\",\n",
       " '',\n",
       " '  An Apothecary.',\n",
       " '',\n",
       " '  Three Musicians.',\n",
       " '',\n",
       " '  An Officer.',\n",
       " '',\n",
       " '',\n",
       " '  Lady Montague, wife to Montague.',\n",
       " '',\n",
       " '  Lady Capulet, wife to Capulet.',\n",
       " '',\n",
       " '  Juliet, daughter to Capulet.',\n",
       " '',\n",
       " '  Nurse to Juliet.',\n",
       " '',\n",
       " '',\n",
       " '  Citizens of Verona; Gentlemen and Gentlewomen of both houses;',\n",
       " '    Maskers, Torchbearers, Pages, Guards, Watchmen, Servants, and',\n",
       " '    Attendants.',\n",
       " '',\n",
       " '                            SCENE.--Verona; Mantua.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '                        THE PROLOGUE',\n",
       " '',\n",
       " '                        Enter Chorus.',\n",
       " '',\n",
       " '',\n",
       " '  Chor. Two households, both alike in dignity,',\n",
       " '    In fair Verona, where we lay our scene,',\n",
       " '    From ancient grudge break to new mutiny,',\n",
       " '    Where civil blood makes civil hands unclean.',\n",
       " '    From forth the fatal loins of these two foes',\n",
       " \"    A pair of star-cross'd lovers take their life;\",\n",
       " \"    Whose misadventur'd piteous overthrows\",\n",
       " \"    Doth with their death bury their parents' strife.\",\n",
       " \"    The fearful passage of their death-mark'd love,\",\n",
       " \"    And the continuance of their parents' rage,\",\n",
       " \"    Which, but their children's end, naught could remove,\",\n",
       " \"    Is now the two hours' traffic of our stage;\",\n",
       " '    The which if you with patient ears attend,',\n",
       " '    What here shall miss, our toil shall strive to mend.',\n",
       " '                                                         [Exit.]',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'ACT I. Scene I.',\n",
       " 'Verona. A public place.',\n",
       " '',\n",
       " 'Enter Sampson and Gregory (with swords and bucklers) of the house',\n",
       " 'of Capulet.',\n",
       " '',\n",
       " '',\n",
       " \"  Samp. Gregory, on my word, we'll not carry coals.\",\n",
       " '',\n",
       " '  Greg. No, for then we should be colliers.',\n",
       " '',\n",
       " \"  Samp. I mean, an we be in choler, we'll draw.\",\n",
       " '',\n",
       " '  Greg. Ay, while you live, draw your neck out of collar.',\n",
       " '',\n",
       " '  Samp. I strike quickly, being moved.',\n",
       " '',\n",
       " '  Greg. But thou art not quickly moved to strike.',\n",
       " '',\n",
       " '  Samp. A dog of the house of Montague moves me.',\n",
       " '',\n",
       " '  Greg. To move is to stir, and to be valiant is to stand.',\n",
       " \"    Therefore, if thou art moved, thou runn'st away.\",\n",
       " '',\n",
       " '  Samp. A dog of that house shall move me to stand. I will take',\n",
       " \"    the wall of any man or maid of Montague's.\",\n",
       " '',\n",
       " '  Greg. That shows thee a weak slave; for the weakest goes to the',\n",
       " '    wall.',\n",
       " '',\n",
       " \"  Samp. 'Tis true; and therefore women, being the weaker vessels,\",\n",
       " \"    are ever thrust to the wall. Therefore I will push Montague's men\",\n",
       " '    from the wall and thrust his maids to the wall.',\n",
       " '',\n",
       " '  Greg. The quarrel is between our masters and us their men.',\n",
       " '',\n",
       " \"  Samp. 'Tis all one. I will show myself a tyrant. When I have\",\n",
       " '    fought with the men, I will be cruel with the maids- I will cut off',\n",
       " '    their heads.',\n",
       " '',\n",
       " '  Greg. The heads of the maids?',\n",
       " '',\n",
       " '  Samp. Ay, the heads of the maids, or their maidenheads.',\n",
       " '    Take it in what sense thou wilt.',\n",
       " '',\n",
       " '  Greg. They must take it in sense that feel it.',\n",
       " '',\n",
       " \"  Samp. Me they shall feel while I am able to stand; and 'tis known I\",\n",
       " '    am a pretty piece of flesh.',\n",
       " '',\n",
       " \"  Greg. 'Tis well thou art not fish; if thou hadst, thou hadst\",\n",
       " '    been poor-John. Draw thy tool! Here comes two of the house of',\n",
       " '    Montagues.',\n",
       " '',\n",
       " '           Enter two other Servingmen [Abram and Balthasar].',\n",
       " '',\n",
       " '',\n",
       " '  Samp. My naked weapon is out. Quarrel! I will back thee.',\n",
       " '',\n",
       " '  Greg. How? turn thy back and run?',\n",
       " '',\n",
       " '  Samp. Fear me not.',\n",
       " '',\n",
       " '  Greg. No, marry. I fear thee!',\n",
       " '',\n",
       " '  Samp. Let us take the law of our sides; let them begin.',\n",
       " '',\n",
       " '  Greg. I will frown as I pass by, and let them take it as they list.',\n",
       " '',\n",
       " '  Samp. Nay, as they dare. I will bite my thumb at them; which is',\n",
       " '    disgrace to them, if they bear it.',\n",
       " '',\n",
       " '  Abr. Do you bite your thumb at us, sir?',\n",
       " '',\n",
       " '  Samp. I do bite my thumb, sir.',\n",
       " '',\n",
       " '  Abr. Do you bite your thumb at us, sir?',\n",
       " '',\n",
       " '  Samp. [aside to Gregory] Is the law of our side if I say ay?',\n",
       " '',\n",
       " '  Greg. [aside to Sampson] No.',\n",
       " '',\n",
       " '  Samp. No, sir, I do not bite my thumb at you, sir; but I bite my',\n",
       " '    thumb, sir.',\n",
       " '',\n",
       " '  Greg. Do you quarrel, sir?',\n",
       " '',\n",
       " '  Abr. Quarrel, sir? No, sir.',\n",
       " '',\n",
       " '  Samp. But if you do, sir, am for you. I serve as good a man as',\n",
       " '    you.',\n",
       " '',\n",
       " '  Abr. No better.',\n",
       " '',\n",
       " '  Samp. Well, sir.',\n",
       " '',\n",
       " '                        Enter Benvolio.',\n",
       " '',\n",
       " '',\n",
       " \"  Greg. [aside to Sampson] Say 'better.' Here comes one of my\",\n",
       " \"    master's kinsmen.\",\n",
       " '',\n",
       " '  Samp. Yes, better, sir.',\n",
       " '',\n",
       " '  Abr. You lie.',\n",
       " '',\n",
       " '  Samp. Draw, if you be men. Gregory, remember thy swashing blow.',\n",
       " '                                                     They fight.',\n",
       " '',\n",
       " '  Ben. Part, fools! [Beats down their swords.]',\n",
       " '    Put up your swords. You know not what you do.',\n",
       " '',\n",
       " '                          Enter Tybalt.',\n",
       " '',\n",
       " '',\n",
       " '  Tyb. What, art thou drawn among these heartless hinds?',\n",
       " '    Turn thee Benvolio! look upon thy death.',\n",
       " '',\n",
       " '  Ben. I do but keep the peace. Put up thy sword,',\n",
       " '    Or manage it to part these men with me.',\n",
       " '',\n",
       " '  Tyb. What, drawn, and talk of peace? I hate the word',\n",
       " '    As I hate hell, all Montagues, and thee.',\n",
       " '    Have at thee, coward!                            They fight.',\n",
       " '',\n",
       " '     Enter an officer, and three or four Citizens with clubs or',\n",
       " '                          partisans.',\n",
       " '',\n",
       " '',\n",
       " '  Officer. Clubs, bills, and partisans! Strike! beat them down!',\n",
       " '',\n",
       " '  Citizens. Down with the Capulets! Down with the Montagues!',\n",
       " '',\n",
       " '           Enter Old Capulet in his gown, and his Wife.',\n",
       " '',\n",
       " '',\n",
       " '  Cap. What noise is this? Give me my long sword, ho!',\n",
       " '',\n",
       " '  Wife. A crutch, a crutch! Why call you for a sword?',\n",
       " '',\n",
       " '  Cap. My sword, I say! Old Montague is come',\n",
       " '    And flourishes his blade in spite of me.',\n",
       " '',\n",
       " '                 Enter Old Montague and his Wife.',\n",
       " '',\n",
       " '',\n",
       " '  Mon. Thou villain Capulet!- Hold me not, let me go.',\n",
       " '',\n",
       " '  M. Wife. Thou shalt not stir one foot to seek a foe.',\n",
       " '',\n",
       " '                Enter Prince Escalus, with his Train.',\n",
       " '',\n",
       " '',\n",
       " '  Prince. Rebellious subjects, enemies to peace,',\n",
       " '    Profaners of this neighbour-stained steel-',\n",
       " '    Will they not hear? What, ho! you men, you beasts,',\n",
       " '    That quench the fire of your pernicious rage',\n",
       " '    With purple fountains issuing from your veins!',\n",
       " '    On pain of torture, from those bloody hands',\n",
       " '    Throw your mistempered weapons to the ground',\n",
       " '    And hear the sentence of your moved prince.',\n",
       " '    Three civil brawls, bred of an airy word',\n",
       " '    By thee, old Capulet, and Montague,',\n",
       " \"    Have thrice disturb'd the quiet of our streets\",\n",
       " \"    And made Verona's ancient citizens\",\n",
       " '    Cast by their grave beseeming ornaments',\n",
       " '    To wield old partisans, in hands as old,',\n",
       " \"    Cank'red with peace, to part your cank'red hate.\",\n",
       " '    If ever you disturb our streets again,',\n",
       " '    Your lives shall pay the forfeit of the peace.',\n",
       " '    For this time all the rest depart away.',\n",
       " '    You, Capulet, shall go along with me;',\n",
       " '    And, Montague, come you this afternoon,',\n",
       " '    To know our farther pleasure in this case,',\n",
       " '    To old Freetown, our common judgment place.',\n",
       " '    Once more, on pain of death, all men depart.',\n",
       " '              Exeunt [all but Montague, his Wife, and Benvolio].',\n",
       " '',\n",
       " '  Mon. Who set this ancient quarrel new abroach?',\n",
       " '    Speak, nephew, were you by when it began?',\n",
       " '',\n",
       " '  Ben. Here were the servants of your adversary',\n",
       " '    And yours, close fighting ere I did approach.',\n",
       " '    I drew to part them. In the instant came',\n",
       " \"    The fiery Tybalt, with his sword prepar'd;\",\n",
       " \"    Which, as he breath'd defiance to my ears,\",\n",
       " '    He swung about his head and cut the winds,',\n",
       " \"    Who, nothing hurt withal, hiss'd him in scorn.\",\n",
       " '    While we were interchanging thrusts and blows,',\n",
       " '    Came more and more, and fought on part and part,',\n",
       " '    Till the Prince came, who parted either part.',\n",
       " '',\n",
       " '  M. Wife. O, where is Romeo? Saw you him to-day?',\n",
       " '    Right glad I am he was not at this fray.',\n",
       " '',\n",
       " \"  Ben. Madam, an hour before the worshipp'd sun\",\n",
       " \"    Peer'd forth the golden window of the East,\",\n",
       " '    A troubled mind drave me to walk abroad;',\n",
       " '    Where, underneath the grove of sycamore',\n",
       " \"    That westward rooteth from the city's side,\",\n",
       " '    So early walking did I see your son.',\n",
       " '    Towards him I made; but he was ware of me',\n",
       " '    And stole into the covert of the wood.',\n",
       " '    I- measuring his affections by my own,',\n",
       " '    Which then most sought where most might not be found,',\n",
       " '    Being one too many by my weary self-',\n",
       " \"    Pursu'd my humour, not Pursuing his,\",\n",
       " \"    And gladly shunn'd who gladly fled from me.\",\n",
       " '',\n",
       " '  Mon. Many a morning hath he there been seen,',\n",
       " \"    With tears augmenting the fresh morning's dew,\",\n",
       " '    Adding to clouds more clouds with his deep sighs;',\n",
       " '    But all so soon as the all-cheering sun',\n",
       " '    Should in the furthest East bean to draw',\n",
       " \"    The shady curtains from Aurora's bed,\",\n",
       " '    Away from light steals home my heavy son',\n",
       " '    And private in his chamber pens himself,',\n",
       " '    Shuts up his windows, locks fair daylight',\n",
       " '    And makes himself an artificial night.',\n",
       " '    Black and portentous must this humour prove',\n",
       " '    Unless good counsel may the cause remove.',\n",
       " '',\n",
       " '  Ben. My noble uncle, do you know the cause?',\n",
       " '',\n",
       " '  Mon. I neither know it nor can learn of him',\n",
       " '',\n",
       " \"  Ben. Have you importun'd him by any means?\",\n",
       " '',\n",
       " '  Mon. Both by myself and many other friend;',\n",
       " \"    But he, his own affections' counsellor,\",\n",
       " '    Is to himself- I will not say how true-',\n",
       " '    But to himself so secret and so close,',\n",
       " '    So far from sounding and discovery,',\n",
       " '    As is the bud bit with an envious worm',\n",
       " '    Ere he can spread his sweet leaves to the air',\n",
       " '    Or dedicate his beauty to the sun.',\n",
       " '    Could we but learn from whence his sorrows grow,',\n",
       " '    We would as willingly give cure as know.',\n",
       " '',\n",
       " '                       Enter Romeo.',\n",
       " '',\n",
       " '',\n",
       " '  Ben. See, where he comes. So please you step aside,',\n",
       " \"    I'll know his grievance, or be much denied.\",\n",
       " '',\n",
       " '  Mon. I would thou wert so happy by thy stay',\n",
       " \"    To hear true shrift. Come, madam, let's away,\",\n",
       " '                                     Exeunt [Montague and Wife].',\n",
       " '',\n",
       " '  Ben. Good morrow, cousin.',\n",
       " '',\n",
       " '  Rom. Is the day so young?',\n",
       " '',\n",
       " '  Ben. But new struck nine.',\n",
       " '',\n",
       " '  Rom. Ay me! sad hours seem long.',\n",
       " '    Was that my father that went hence so fast?',\n",
       " '',\n",
       " \"  Ben. It was. What sadness lengthens Romeo's hours?\",\n",
       " '',\n",
       " '  Rom. Not having that which having makes them short.',\n",
       " '',\n",
       " '  Ben. In love?',\n",
       " '',\n",
       " '  Rom. Out-',\n",
       " '',\n",
       " '  Ben. Of love?',\n",
       " '',\n",
       " '  Rom. Out of her favour where I am in love.',\n",
       " '',\n",
       " '  Ben. Alas that love, so gentle in his view,',\n",
       " '    Should be so tyrannous and rough in proof!',\n",
       " '',\n",
       " '  Rom. Alas that love, whose view is muffled still,',\n",
       " '    Should without eyes see pathways to his will!',\n",
       " '    Where shall we dine? O me! What fray was here?',\n",
       " '    Yet tell me not, for I have heard it all.',\n",
       " \"    Here's much to do with hate, but more with love.\",\n",
       " '    Why then, O brawling love! O loving hate!',\n",
       " '    O anything, of nothing first create!',\n",
       " '    O heavy lightness! serious vanity!',\n",
       " '    Misshapen chaos of well-seeming forms!',\n",
       " '    Feather of lead, bright smoke, cold fire, sick health!',\n",
       " '    Still-waking sleep, that is not what it is',\n",
       " '    This love feel I, that feel no love in this.',\n",
       " '    Dost thou not laugh?',\n",
       " '',\n",
       " '  Ben. No, coz, I rather weep.',\n",
       " '',\n",
       " '  Rom. Good heart, at what?',\n",
       " '',\n",
       " \"  Ben. At thy good heart's oppression.\",\n",
       " '',\n",
       " \"  Rom. Why, such is love's transgression.\",\n",
       " '    Griefs of mine own lie heavy in my breast,',\n",
       " '    Which thou wilt propagate, to have it prest',\n",
       " '    With more of thine. This love that thou hast shown',\n",
       " '    Doth add more grief to too much of mine own.',\n",
       " \"    Love is a smoke rais'd with the fume of sighs;\",\n",
       " \"    Being purg'd, a fire sparkling in lovers' eyes;\",\n",
       " \"    Being vex'd, a sea nourish'd with lovers' tears.\",\n",
       " '    What is it else? A madness most discreet,',\n",
       " '    A choking gall, and a preserving sweet.',\n",
       " '    Farewell, my coz.',\n",
       " '',\n",
       " '  Ben. Soft! I will go along.',\n",
       " '    An if you leave me so, you do me wrong.',\n",
       " '',\n",
       " '  Rom. Tut! I have lost myself; I am not here:',\n",
       " \"    This is not Romeo, he's some other where.\",\n",
       " '',\n",
       " '  Ben. Tell me in sadness, who is that you love?',\n",
       " '',\n",
       " '  Rom. What, shall I groan and tell thee?',\n",
       " '',\n",
       " '  Ben. Groan? Why, no;',\n",
       " '    But sadly tell me who.',\n",
       " '',\n",
       " '  Rom. Bid a sick man in sadness make his will.',\n",
       " \"    Ah, word ill urg'd to one that is so ill!\",\n",
       " '    In sadness, cousin, I do love a woman.',\n",
       " '',\n",
       " \"  Ben. I aim'd so near when I suppos'd you lov'd.\",\n",
       " '',\n",
       " \"  Rom. A right good markman! And she's fair I love.\",\n",
       " '',\n",
       " '  Ben. A right fair mark, fair coz, is soonest hit.',\n",
       " '',\n",
       " \"  Rom. Well, in that hit you miss. She'll not be hit\",\n",
       " \"    With Cupid's arrow. She hath Dian's wit,\",\n",
       " \"    And, in strong proof of chastity well arm'd,\",\n",
       " \"    From Love's weak childish bow she lives unharm'd.\",\n",
       " '    She will not stay the siege of loving terms,',\n",
       " \"    Nor bide th' encounter of assailing eyes,\",\n",
       " '    Nor ope her lap to saint-seducing gold.',\n",
       " \"    O, she's rich in beauty; only poor\",\n",
       " '    That, when she dies, with beauty dies her store.',\n",
       " '',\n",
       " '  Ben. Then she hath sworn that she will still live chaste?',\n",
       " '',\n",
       " '  Rom. She hath, and in that sparing makes huge waste;',\n",
       " \"    For beauty, starv'd with her severity,\",\n",
       " '    Cuts beauty off from all posterity.',\n",
       " '    She is too fair, too wise, wisely too fair,',\n",
       " '    To merit bliss by making me despair.',\n",
       " '    She hath forsworn to love, and in that vow',\n",
       " '    Do I live dead that live to tell it now.',\n",
       " '',\n",
       " \"  Ben. Be rul'd by me: forget to think of her.\",\n",
       " '',\n",
       " '  Rom. O, teach me how I should forget to think!',\n",
       " '',\n",
       " '  Ben. By giving liberty unto thine eyes.',\n",
       " '    Examine other beauties.',\n",
       " '',\n",
       " \"  Rom. 'Tis the way\",\n",
       " '    To call hers (exquisite) in question more.',\n",
       " \"    These happy masks that kiss fair ladies' brows,\",\n",
       " '    Being black puts us in mind they hide the fair.',\n",
       " '    He that is strucken blind cannot forget',\n",
       " '    The precious treasure of his eyesight lost.',\n",
       " '    Show me a mistress that is passing fair,',\n",
       " '    What doth her beauty serve but as a note',\n",
       " \"    Where I may read who pass'd that passing fair?\",\n",
       " '    Farewell. Thou canst not teach me to forget.',\n",
       " '',\n",
       " \"  Ben. I'll pay that doctrine, or else die in debt.      Exeunt.\",\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Scene II.',\n",
       " 'A Street.',\n",
       " '',\n",
       " 'Enter Capulet, County Paris, and [Servant] -the Clown.',\n",
       " '',\n",
       " '',\n",
       " '  Cap. But Montague is bound as well as I,',\n",
       " \"    In penalty alike; and 'tis not hard, I think,\",\n",
       " '    For men so old as we to keep the peace.',\n",
       " '',\n",
       " '  Par. Of honourable reckoning are you both,',\n",
       " \"    And pity 'tis you liv'd at odds so long.\",\n",
       " '    But now, my lord, what say you to my suit?',\n",
       " '',\n",
       " \"  Cap. But saying o'er what I have said before:\",\n",
       " '    My child is yet a stranger in the world,',\n",
       " '    She hath not seen the change of fourteen years;',\n",
       " '    Let two more summers wither in their pride',\n",
       " '    Ere we may think her ripe to be a bride.',\n",
       " '',\n",
       " '  Par. Younger than she are happy mothers made.',\n",
       " '',\n",
       " \"  Cap. And too soon marr'd are those so early made.\",\n",
       " '    The earth hath swallowed all my hopes but she;',\n",
       " '    She is the hopeful lady of my earth.',\n",
       " '    But woo her, gentle Paris, get her heart;',\n",
       " '    My will to her consent is but a part.',\n",
       " '    An she agree, within her scope of choice',\n",
       " '    Lies my consent and fair according voice.',\n",
       " \"    This night I hold an old accustom'd feast,\",\n",
       " '    Whereto I have invited many a guest,',\n",
       " '    Such as I love; and you among the store,',\n",
       " '    One more, most welcome, makes my number more.',\n",
       " '    At my poor house look to behold this night',\n",
       " '    Earth-treading stars that make dark heaven light.',\n",
       " '    Such comfort as do lusty young men feel',\n",
       " \"    When well apparell'd April on the heel\",\n",
       " '    Of limping Winter treads, even such delight',\n",
       " '    Among fresh female buds shall you this night',\n",
       " '    Inherit at my house. Hear all, all see,',\n",
       " '    And like her most whose merit most shall be;',\n",
       " '    Which, on more view of many, mine, being one,',\n",
       " \"    May stand in number, though in reck'ning none.\",\n",
       " '    Come, go with me. [To Servant, giving him a paper] Go,',\n",
       " '    sirrah, trudge about',\n",
       " '    Through fair Verona; find those persons out',\n",
       " '    Whose names are written there, and to them say,',\n",
       " '    My house and welcome on their pleasure stay-',\n",
       " '                                     Exeunt [Capulet and Paris].',\n",
       " '',\n",
       " '  Serv. Find them out whose names are written here? It is written',\n",
       " '    that the shoemaker should meddle with his yard and the tailor',\n",
       " '    with his last, the fisher with his pencil and the painter',\n",
       " '    with his nets; but I am sent to find those persons whose names are',\n",
       " '    here writ, and can never find what names the writing person',\n",
       " '    hath here writ. I must to the learned. In good time!',\n",
       " '',\n",
       " '                   Enter Benvolio and Romeo.',\n",
       " '',\n",
       " '',\n",
       " \"  Ben. Tut, man, one fire burns out another's burning;\",\n",
       " \"    One pain is lessoned by another's anguish;\",\n",
       " '    Turn giddy, and be holp by backward turning;',\n",
       " \"    One desperate grief cures with another's languish.\",\n",
       " '    Take thou some new infection to thy eye,',\n",
       " '    And the rank poison of the old will die.',\n",
       " '',\n",
       " '  Rom. Your plantain leaf is excellent for that.',\n",
       " '',\n",
       " '  Ben. For what, I pray thee?',\n",
       " '',\n",
       " '  Rom. For your broken shin.',\n",
       " '',\n",
       " '  Ben. Why, Romeo, art thou mad?',\n",
       " '',\n",
       " '  Rom. Not mad, but bound more than a madman is;',\n",
       " '    Shut up in Prison, kept without my food,',\n",
       " \"    Whipp'd and tormented and- God-den, good fellow.\",\n",
       " '',\n",
       " \"  Serv. God gi' go-den. I pray, sir, can you read?\",\n",
       " '',\n",
       " '  Rom. Ay, mine own fortune in my misery.',\n",
       " '',\n",
       " '  Serv. Perhaps you have learned it without book. But I pray, can',\n",
       " '    you read anything you see?',\n",
       " '',\n",
       " '  Rom. Ay, If I know the letters and the language.',\n",
       " '',\n",
       " '  Serv. Ye say honestly. Rest you merry!',\n",
       " '',\n",
       " '  Rom. Stay, fellow; I can read.                       He reads.',\n",
       " '',\n",
       " \"      'Signior Martino and his wife and daughters;\",\n",
       " '      County Anselmo and his beauteous sisters;',\n",
       " '      The lady widow of Vitruvio;',\n",
       " '      Signior Placentio and His lovely nieces;',\n",
       " '      Mercutio and his brother Valentine;',\n",
       " '      Mine uncle Capulet, his wife, and daughters;',\n",
       " '      My fair niece Rosaline and Livia;',\n",
       " '      Signior Valentio and His cousin Tybalt;',\n",
       " \"      Lucio and the lively Helena.'\",\n",
       " '',\n",
       " '    [Gives back the paper.] A fair assembly. Whither should they',\n",
       " '    come?',\n",
       " '',\n",
       " '  Serv. Up.',\n",
       " '',\n",
       " '  Rom. Whither?',\n",
       " '',\n",
       " '  Serv. To supper, to our house.',\n",
       " '',\n",
       " '  Rom. Whose house?',\n",
       " '',\n",
       " \"  Serv. My master's.\",\n",
       " '',\n",
       " \"  Rom. Indeed I should have ask'd you that before.\",\n",
       " '',\n",
       " \"  Serv. Now I'll tell you without asking. My master is the great\",\n",
       " '    rich Capulet; and if you be not of the house of Montagues, I pray',\n",
       " '    come and crush a cup of wine. Rest you merry!               Exit.',\n",
       " '',\n",
       " \"  Ben. At this same ancient feast of Capulet's\",\n",
       " \"    Sups the fair Rosaline whom thou so lov'st;\",\n",
       " '    With all the admired beauties of Verona.',\n",
       " '    Go thither, and with unattainted eye',\n",
       " '    Compare her face with some that I shall show,',\n",
       " '    And I will make thee think thy swan a crow.',\n",
       " '',\n",
       " '  Rom. When the devout religion of mine eye',\n",
       " '    Maintains such falsehood, then turn tears to fires;',\n",
       " \"    And these, who, often drown'd, could never die,\",\n",
       " '    Transparent heretics, be burnt for liars!',\n",
       " '    One fairer than my love? The all-seeing sun',\n",
       " \"    Ne'er saw her match since first the world begun.\",\n",
       " '',\n",
       " '  Ben. Tut! you saw her fair, none else being by,',\n",
       " \"    Herself pois'd with herself in either eye;\",\n",
       " \"    But in that crystal scales let there be weigh'd\",\n",
       " \"    Your lady's love against some other maid\",\n",
       " '    That I will show you shining at this feast,',\n",
       " '    And she shall scant show well that now seems best.',\n",
       " '',\n",
       " \"  Rom. I'll go along, no such sight to be shown,\",\n",
       " '    But to rejoice in splendour of my own.              [Exeunt.]',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Scene III.',\n",
       " \"Capulet's house.\",\n",
       " '',\n",
       " \"Enter Capulet's Wife, and Nurse.\",\n",
       " '',\n",
       " '',\n",
       " \"  Wife. Nurse, where's my daughter? Call her forth to me.\",\n",
       " '',\n",
       " '  Nurse. Now, by my maidenhead at twelve year old,',\n",
       " '    I bade her come. What, lamb! what ladybird!',\n",
       " \"    God forbid! Where's this girl? What, Juliet!\",\n",
       " '',\n",
       " '                         Enter Juliet.',\n",
       " '',\n",
       " '',\n",
       " '  Jul. How now? Who calls?',\n",
       " '',\n",
       " '  Nurse. Your mother.',\n",
       " '',\n",
       " '  Jul. Madam, I am here.',\n",
       " '    What is your will?',\n",
       " '',\n",
       " '  Wife. This is the matter- Nurse, give leave awhile,',\n",
       " '    We must talk in secret. Nurse, come back again;',\n",
       " \"    I have rememb'red me, thou's hear our counsel.\",\n",
       " \"    Thou knowest my daughter's of a pretty age.\",\n",
       " '',\n",
       " '  Nurse. Faith, I can tell her age unto an hour.',\n",
       " '',\n",
       " \"  Wife. She's not fourteen.\",\n",
       " '',\n",
       " \"  Nurse. I'll lay fourteen of my teeth-\",\n",
       " '    And yet, to my teen be it spoken, I have but four-',\n",
       " '    She is not fourteen. How long is it now',\n",
       " '    To Lammastide?',\n",
       " '',\n",
       " '  Wife. A fortnight and odd days.',\n",
       " '',\n",
       " '  Nurse. Even or odd, of all days in the year,',\n",
       " '    Come Lammas Eve at night shall she be fourteen.',\n",
       " '    Susan and she (God rest all Christian souls!)',\n",
       " '    Were of an age. Well, Susan is with God;',\n",
       " '    She was too good for me. But, as I said,',\n",
       " '    On Lammas Eve at night shall she be fourteen;',\n",
       " '    That shall she, marry; I remember it well.',\n",
       " \"    'Tis since the earthquake now eleven years;\",\n",
       " \"    And she was wean'd (I never shall forget it),\",\n",
       " '    Of all the days of the year, upon that day;',\n",
       " '    For I had then laid wormwood to my dug,',\n",
       " '    Sitting in the sun under the dovehouse wall.',\n",
       " '    My lord and you were then at Mantua.',\n",
       " '    Nay, I do bear a brain. But, as I said,',\n",
       " '    When it did taste the wormwood on the nipple',\n",
       " '    Of my dug and felt it bitter, pretty fool,',\n",
       " '    To see it tetchy and fall out with the dug!',\n",
       " \"    Shake, quoth the dovehouse! 'Twas no need, I trow,\",\n",
       " '    To bid me trudge.',\n",
       " '    And since that time it is eleven years,',\n",
       " \"    For then she could stand high-lone; nay, by th' rood,\",\n",
       " '    She could have run and waddled all about;',\n",
       " '    For even the day before, she broke her brow;',\n",
       " '    And then my husband (God be with his soul!',\n",
       " \"    'A was a merry man) took up the child.\",\n",
       " \"    'Yea,' quoth he, 'dost thou fall upon thy face?\",\n",
       " '    Thou wilt fall backward when thou hast more wit;',\n",
       " \"    Wilt thou not, Jule?' and, by my holidam,\",\n",
       " \"    The pretty wretch left crying, and said 'Ay.'\",\n",
       " '    To see now how a jest shall come about!',\n",
       " '    I warrant, an I should live a thousand yeas,',\n",
       " \"    I never should forget it. 'Wilt thou not, Jule?' quoth he,\",\n",
       " \"    And, pretty fool, it stinted, and said 'Ay.'\",\n",
       " '',\n",
       " '  Wife. Enough of this. I pray thee hold thy peace.',\n",
       " '',\n",
       " '  Nurse. Yes, madam. Yet I cannot choose but laugh',\n",
       " \"    To think it should leave crying and say 'Ay.'\",\n",
       " '    And yet, I warrant, it bad upon it brow',\n",
       " \"    A bump as big as a young cock'rel's stone;\",\n",
       " '    A perilous knock; and it cried bitterly.',\n",
       " \"    'Yea,' quoth my husband, 'fall'st upon thy face?\",\n",
       " '    Thou wilt fall backward when thou comest to age;',\n",
       " \"    Wilt thou not, Jule?' It stinted, and said 'Ay.'\",\n",
       " '',\n",
       " '  Jul. And stint thou too, I pray thee, nurse, say I.',\n",
       " '',\n",
       " '  Nurse. Peace, I have done. God mark thee to his grace!',\n",
       " \"    Thou wast the prettiest babe that e'er I nurs'd.\",\n",
       " '    An I might live to see thee married once, I have my wish.',\n",
       " '',\n",
       " \"  Wife. Marry, that 'marry' is the very theme\",\n",
       " '    I came to talk of. Tell me, daughter Juliet,',\n",
       " '    How stands your disposition to be married?',\n",
       " '',\n",
       " '  Jul. It is an honour that I dream not of.',\n",
       " '',\n",
       " '  Nurse. An honour? Were not I thine only nurse,',\n",
       " \"    I would say thou hadst suck'd wisdom from thy teat.\",\n",
       " '',\n",
       " '  Wife. Well, think of marriage now. Younger than you,',\n",
       " '    Here in Verona, ladies of esteem,',\n",
       " '    Are made already mothers. By my count,',\n",
       " '    I was your mother much upon these years',\n",
       " '    That you are now a maid. Thus then in brief:',\n",
       " '    The valiant Paris seeks you for his love.',\n",
       " '',\n",
       " '  Nurse. A man, young lady! lady, such a man',\n",
       " \"    As all the world- why he's a man of wax.\",\n",
       " '',\n",
       " \"  Wife. Verona's summer hath not such a flower.\",\n",
       " '',\n",
       " \"  Nurse. Nay, he's a flower, in faith- a very flower.\",\n",
       " '',\n",
       " '  Wife. What say you? Can you love the gentleman?',\n",
       " '    This night you shall behold him at our feast.',\n",
       " \"    Read o'er the volume of young Paris' face,\",\n",
       " \"    And find delight writ there with beauty's pen;\",\n",
       " '    Examine every married lineament,',\n",
       " '    And see how one another lends content;',\n",
       " \"    And what obscur'd in this fair volume lies\",\n",
       " '    Find written in the margent of his eyes,',\n",
       " '    This precious book of love, this unbound lover,',\n",
       " '    To beautify him only lacks a cover.',\n",
       " \"    The fish lives in the sea, and 'tis much pride\",\n",
       " '    For fair without the fair within to hide.',\n",
       " \"    That book in many's eyes doth share the glory,\",\n",
       " '    That in gold clasps locks in the golden story;',\n",
       " '    So shall you share all that he doth possess,',\n",
       " '    By having him making yourself no less.',\n",
       " '',\n",
       " '  Nurse. No less? Nay, bigger! Women grow by men',\n",
       " '',\n",
       " \"  Wife. Speak briefly, can you like of Paris' love?\",\n",
       " '',\n",
       " \"  Jul. I'll look to like, if looking liking move;\",\n",
       " '    But no more deep will I endart mine eye',\n",
       " '    Than your consent gives strength to make it fly.',\n",
       " '',\n",
       " '                        Enter Servingman.',\n",
       " '',\n",
       " '',\n",
       " \"  Serv. Madam, the guests are come, supper serv'd up, you call'd,\",\n",
       " \"    my young lady ask'd for, the nurse curs'd in the pantry, and\",\n",
       " '    everything in extremity. I must hence to wait. I beseech you',\n",
       " '    follow straight.',\n",
       " '',\n",
       " '  Wife. We follow thee.                       Exit [Servingman].',\n",
       " '    Juliet, the County stays.',\n",
       " '',\n",
       " '  Nurse. Go, girl, seek happy nights to happy days.',\n",
       " '                                                         Exeunt.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Scene IV.',\n",
       " 'A street.',\n",
       " '',\n",
       " 'Enter Romeo, Mercutio, Benvolio, with five or six other Maskers;',\n",
       " 'Torchbearers.',\n",
       " '',\n",
       " '',\n",
       " '  Rom. What, shall this speech be spoke for our excuse?',\n",
       " '    Or shall we on without apology?',\n",
       " '',\n",
       " '  Ben. The date is out of such prolixity.',\n",
       " \"    We'll have no Cupid hoodwink'd with a scarf,\",\n",
       " \"    Bearing a Tartar's painted bow of lath,\",\n",
       " '    Scaring the ladies like a crowkeeper;',\n",
       " '    Nor no without-book prologue, faintly spoke',\n",
       " '    After the prompter, for our entrance;',\n",
       " '    But, let them measure us by what they will,',\n",
       " \"    We'll measure them a measure, and be gone.\",\n",
       " '',\n",
       " '  Rom. Give me a torch. I am not for this ambling.',\n",
       " '    Being but heavy, I will bear the light.',\n",
       " '',\n",
       " '  Mer. Nay, gentle Romeo, we must have you dance.',\n",
       " '',\n",
       " '  Rom. Not I, believe me. You have dancing shoes',\n",
       " '    With nimble soles; I have a soul of lead',\n",
       " '    So stakes me to the ground I cannot move.',\n",
       " '',\n",
       " \"  Mer. You are a lover. Borrow Cupid's wings\",\n",
       " '    And soar with them above a common bound.',\n",
       " '',\n",
       " '  Rom. I am too sore enpierced with his shaft',\n",
       " '    To soar with his light feathers; and so bound',\n",
       " '    I cannot bound a pitch above dull woe.',\n",
       " \"    Under love's heavy burthen do I sink.\",\n",
       " '',\n",
       " '  Mer. And, to sink in it, should you burthen love-',\n",
       " '    Too great oppression for a tender thing.',\n",
       " '',\n",
       " '  Rom. Is love a tender thing? It is too rough,',\n",
       " \"    Too rude, too boist'rous, and it pricks like thorn.\",\n",
       " '',\n",
       " '  Mer. If love be rough with you, be rough with love.',\n",
       " '    Prick love for pricking, and you beat love down.',\n",
       " '    Give me a case to put my visage in.',\n",
       " '    A visor for a visor! What care I',\n",
       " '    What curious eye doth quote deformities?',\n",
       " '    Here are the beetle brows shall blush for me.',\n",
       " '',\n",
       " '  Ben. Come, knock and enter; and no sooner in',\n",
       " '    But every man betake him to his legs.',\n",
       " '',\n",
       " '  Rom. A torch for me! Let wantons light of heart',\n",
       " '    Tickle the senseless rushes with their heels;',\n",
       " \"    For I am proverb'd with a grandsire phrase,\",\n",
       " \"    I'll be a candle-holder and look on;\",\n",
       " \"    The game was ne'er so fair, and I am done.\",\n",
       " '',\n",
       " \"  Mer. Tut! dun's the mouse, the constable's own word!\",\n",
       " \"    If thou art Dun, we'll draw thee from the mire\",\n",
       " \"    Of this sir-reverence love, wherein thou stick'st\",\n",
       " '    Up to the ears. Come, we burn daylight, ho!',\n",
       " '',\n",
       " \"  Rom. Nay, that's not so.\",\n",
       " '',\n",
       " '  Mer. I mean, sir, in delay',\n",
       " '    We waste our lights in vain, like lamps by day.',\n",
       " '    Take our good meaning, for our judgment sits',\n",
       " '    Five times in that ere once in our five wits.',\n",
       " '',\n",
       " '  Rom. And we mean well, in going to this masque;',\n",
       " \"    But 'tis no wit to go.\",\n",
       " '',\n",
       " '  Mer. Why, may one ask?',\n",
       " '',\n",
       " '  Rom. I dreamt a dream to-night.',\n",
       " '',\n",
       " '  Mer. And so did I.',\n",
       " '',\n",
       " '  Rom. Well, what was yours?',\n",
       " '',\n",
       " '  Mer. That dreamers often lie.',\n",
       " '',\n",
       " '  Rom. In bed asleep, while they do dream things true.',\n",
       " '',\n",
       " '  Mer. O, then I see Queen Mab hath been with you.',\n",
       " \"    She is the fairies' midwife, and she comes\",\n",
       " '    In shape no bigger than an agate stone',\n",
       " '    On the forefinger of an alderman,',\n",
       " '    Drawn with a team of little atomies',\n",
       " \"    Athwart men's noses as they lie asleep;\",\n",
       " \"    Her wagon spokes made of long spinners' legs,\",\n",
       " '    The cover, of the wings of grasshoppers;',\n",
       " \"    Her traces, of the smallest spider's web;\",\n",
       " \"    Her collars, of the moonshine's wat'ry beams;\",\n",
       " \"    Her whip, of cricket's bone; the lash, of film;\",\n",
       " '    Her wagoner, a small grey-coated gnat,',\n",
       " '    Not half so big as a round little worm',\n",
       " \"    Prick'd from the lazy finger of a maid;\",\n",
       " '    Her chariot is an empty hazelnut,',\n",
       " '    Made by the joiner squirrel or old grub,',\n",
       " \"    Time out o' mind the fairies' coachmakers.\",\n",
       " \"    And in this state she 'gallops night by night\",\n",
       " \"    Through lovers' brains, and then they dream of love;\",\n",
       " \"    O'er courtiers' knees, that dream on cursies straight;\",\n",
       " \"    O'er lawyers' fingers, who straight dream on fees;\",\n",
       " \"    O'er ladies' lips, who straight on kisses dream,\",\n",
       " '    Which oft the angry Mab with blisters plagues,',\n",
       " '    Because their breaths with sweetmeats tainted are.',\n",
       " \"    Sometime she gallops o'er a courtier's nose,\",\n",
       " '    And then dreams he of smelling out a suit;',\n",
       " \"    And sometime comes she with a tithe-pig's tail\",\n",
       " \"    Tickling a parson's nose as 'a lies asleep,\",\n",
       " '    Then dreams he of another benefice.',\n",
       " \"    Sometimes she driveth o'er a soldier's neck,\",\n",
       " '    And then dreams he of cutting foreign throats,',\n",
       " '    Of breaches, ambuscadoes, Spanish blades,',\n",
       " '    Of healths five fadom deep; and then anon',\n",
       " '    Drums in his ear, at which he starts and wakes,',\n",
       " '    And being thus frighted, swears a prayer or two',\n",
       " '    And sleeps again. This is that very Mab',\n",
       " '    That plats the manes of horses in the night',\n",
       " '    And bakes the elflocks in foul sluttish, hairs,',\n",
       " '    Which once untangled much misfortune bodes',\n",
       " '    This is the hag, when maids lie on their backs,',\n",
       " '    That presses them and learns them first to bear,',\n",
       " '    Making them women of good carriage.',\n",
       " '    This is she-',\n",
       " '',\n",
       " '  Rom. Peace, peace, Mercutio, peace!',\n",
       " \"    Thou talk'st of nothing.\",\n",
       " '',\n",
       " '  Mer. True, I talk of dreams;',\n",
       " '    Which are the children of an idle brain,',\n",
       " '    Begot of nothing but vain fantasy;',\n",
       " '    Which is as thin of substance as the air,',\n",
       " '    And more inconstant than the wind, who wooes',\n",
       " '    Even now the frozen bosom of the North',\n",
       " \"    And, being anger'd, puffs away from thence,\",\n",
       " '    Turning his face to the dew-dropping South.',\n",
       " '',\n",
       " '  Ben. This wind you talk of blows us from ourselves.',\n",
       " '    Supper is done, and we shall come too late.',\n",
       " '',\n",
       " '  Rom. I fear, too early; for my mind misgives',\n",
       " '    Some consequence, yet hanging in the stars,',\n",
       " '    Shall bitterly begin his fearful date',\n",
       " \"    With this night's revels and expire the term\",\n",
       " \"    Of a despised life, clos'd in my breast,\",\n",
       " '    By some vile forfeit of untimely death.',\n",
       " '    But he that hath the steerage of my course',\n",
       " '    Direct my sail! On, lusty gentlemen!',\n",
       " '',\n",
       " '  Ben. Strike, drum.',\n",
       " '                           They march about the stage. [Exeunt.]',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Scene V.',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#se aplica la acción collect() sobre el RDD podemos ver como fueron cargados los datos.\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de archivos CSV\n",
    "\n",
    "Spark SQL proporciona spark.read.csv(\"path\") para leer un archivo CSV en Spark DataFrame y dataframe.write.csv(\"path\") para guardar o escribir en el archivo CSV. Spark soporta la lectura de archivos con pipes, comas, tabulaciones o cualquier otro delimitador/separador.\n",
    "\n",
    "Nota: Spark admite la lectura de archivos en CSV, JSON, TEXT, Parquet, y muchos más formatos de archivo en Spark DataFrame.\n",
    "\n",
    "### Ejemplo de lectura\n",
    "\n",
    "Vamos a leer el archivo csv ahora usando spark.read.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear un SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Prueba_Leer_CSV') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "#Leer el csv\n",
    "#Header indica que ya tiene una cabecera y el encoding es como se va a codificar, en este caso, latin1 para que pueda leer \n",
    "#las letras ñ o con acentos las palabras\n",
    "df = spark.read.csv(\"C:\\\\Users\\\\jmas_\\\\Jupyter\\\\Datasets\\\\info_sni.csv\", header=True, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comprobar el tipo de dato\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+----------------+---------------+------+--------------------------+--------------------+\n",
      "|Grado|Apellido paterno |Apellido Materno|         Nombre|Nivel |Institución de adcripción |Área de conocimiento|\n",
      "+-----+-----------------+----------------+---------------+------+--------------------------+--------------------+\n",
      "|  DR.|            VERMA|         JAISWAL|   SURENDRA PAL|     3|      UNIVERSIDAD NACIO...|Área I: Físico-Ma...|\n",
      "|  DR.|           ALONSO|         SANCHEZ|          JORGE|     3|      CENTRO DE INVESTI...|Área IV: Humanida...|\n",
      "| DRA.|           AZAOLA|         GARRIDO|          ELENA|     3|      CENTRO DE INVESTI...|Área IV: Humanida...|\n",
      "|  DR.|            RAMON|          ROMERO|  FIDEL ALBERTO|     2|      UNIVERSIDAD NACIO...|Área II: Biología...|\n",
      "|  DR.|            PEREZ|           ANGON|   MIGUEL ANGEL|     3|      CENTRO DE INVESTI...|Área I: Físico-Ma...|\n",
      "|  DR.|           MEDINA|          NOYOLA|      MAGDALENO|     3|      UNIVERSIDAD AUTON...|Área I: Físico-Ma...|\n",
      "|  DR.|            MORAN|           LOPEZ|      JOSE LUIS|     3|      INSTITUTO POTOSIN...|Área I: Físico-Ma...|\n",
      "|  DR.|            LLAMA|            LEAL|   MIGUEL ANGEL|     1|      TECNOLOGICO NACIO...|Área VII: Ingenie...|\n",
      "|  DR.|       SANTIBAÑEZ|          DAVILA|  VICTOR ADRIAN|     3|      TECNOLOGICO NACIO...|Área VII: Ingenie...|\n",
      "|  DR.|             SOTO|       RODRIGUEZ|        ROGELIO|     2|      INSTITUTO TECNOLO...|Área VII: Ingenie...|\n",
      "|  DR.|           VALDES|      PEREZGASGA|      FRANCISCO|     2|      TECNOLOGICO NACIO...|Área VII: Ingenie...|\n",
      "|  DR.|             HAHN|          SCHLAM| FEDERICO FELIX|     2|      UNIVERSIDAD AUTON...|Área VII: Ingenie...|\n",
      "|  DR.|           HUERTA|     QUINTANILLA|        RODRIGO|     3|      CENTRO DE INVESTI...|Área I: Físico-Ma...|\n",
      "|  DR.|            LUCIO|        MARTINEZ|      JOSE LUIS|     3|      UNIVERSIDAD DE GU...|Área I: Físico-Ma...|\n",
      "|  DR.|   HERNANDEZ-VELA|         SALGADO|        EDMUNDO|     3|      UNIVERSIDAD NACIO...|Área V: Ciencias ...|\n",
      "|  DR.|          SANCHEZ|            RUIZ|ENRIQUE ERNESTO|     3|      UNIVERSIDAD DE GU...|Área V: Ciencias ...|\n",
      "|  DR.|          MENDOZA|         ALVAREZ| JULIO GREGORIO|     3|      CENTRO DE INVESTI...|Área I: Físico-Ma...|\n",
      "|  DR.|         GONZALEZ|      DE LA CRUZ| GERARDO ACACIO|     3|      CENTRO DE INVESTI...|Área I: Físico-Ma...|\n",
      "|  DR.|        HERNANDEZ|           LERMA|        ONESIMO|     3|      CENTRO DE INVESTI...|Área I: Físico-Ma...|\n",
      "|  DR.|          SERRANO|         SANTOYO|         ARTURO|     2|      CENTRO DE INVESTI...|Área VII: Ingenie...|\n",
      "+-----+-----------------+----------------+---------------+------+--------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mostrar los registros del CSV (los 20 primeros)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+----------------+-------------+------+--------------------------+--------------------+\n",
      "|Grado|Apellido paterno |Apellido Materno|       Nombre|Nivel |Institución de adcripción |Área de conocimiento|\n",
      "+-----+-----------------+----------------+-------------+------+--------------------------+--------------------+\n",
      "|  DR.|            VERMA|         JAISWAL| SURENDRA PAL|     3|      UNIVERSIDAD NACIO...|Área I: Físico-Ma...|\n",
      "|  DR.|           ALONSO|         SANCHEZ|        JORGE|     3|      CENTRO DE INVESTI...|Área IV: Humanida...|\n",
      "| DRA.|           AZAOLA|         GARRIDO|        ELENA|     3|      CENTRO DE INVESTI...|Área IV: Humanida...|\n",
      "|  DR.|            RAMON|          ROMERO|FIDEL ALBERTO|     2|      UNIVERSIDAD NACIO...|Área II: Biología...|\n",
      "+-----+-----------------+----------------+-------------+------+--------------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mostrar los primeros 4 registros\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion de RDD: vacío y con datos\n",
    "\n",
    "RDD (Resilient Distributed Dataset) es una estructura de datos fundamental de Spark y es la principal abstracción de datos en Apache Spark y el Spark Core. Los RDD son colecciones distribuidas de objetos inmutables y tolerantes a fallos, lo que significa que una vez que se crea un RDD no se puede cambiar. Cada conjunto de datos en RDD se divide en particiones lógicas, que pueden ser calculadas en diferentes nodos del clúster.\n",
    "\n",
    "### Caracteristicas\n",
    "\n",
    "los RDDs son una colección de objetos similar a las colecciones en Scala, con la diferencia de que los RDDs se computan en varias JVMs dispersas en múltiples servidores físicos también llamados nodos en un cluster mientras que una colección en Scala vive en una sola JVM. Por ejemplo, si cargamos un RDD a partir de un archivo plano se creará un RDD de cadenas de texto, una por cada linea del archivo.\n",
    "\n",
    "En Scala o Java, al declarar un RDD se debe definir el tipo de los registros:\n",
    "\n",
    "* Lenguajes estáticos : RDD[String] en Scala y JavaRDD en Java.\n",
    "* Lenguajes dinámicos : Python se pueden mezclar los tipos de datos.\n",
    "\n",
    "RDDs han sido diseñados desde el inicio para ser distribuidos: Los registros que lo componen se repartirán entre los clúster.\n",
    "\n",
    "Los RDDs son inmutables: no se puede modificar ni actualizar. Una vez creado, así permanece hasta que se termina la ejecución del programa\n",
    "\n",
    "Los RDDs de Spark no son muy adecuados para aplicaciones que realizan actualizaciones en el almacén de estado, como los sistemas de almacenamiento de una aplicación web. El objetivo de los RDD es proporcionar un modelo de programación eficiente para el análisis por lotes y dejar estas aplicaciones asíncronas.\n",
    "\n",
    "Un RDD puede estar presente en un solo SparkContext y el RDD puede tener un nombre y un identificador único (id)\n",
    "\n",
    "### Funcionalidad\n",
    "\n",
    "Se pueden aplicar múltiples operaciones sobre estos RDDs para lograr una determinada tarea.\n",
    "\n",
    "Para aplicar operaciones sobre estos RDD's, hay dos formas\n",
    "\n",
    "* **Transformación** - Estas son las operaciones que se aplican sobre un RDD para crear un nuevo RDD. Filtro, groupBy y map son ejemplos de transformaciones.\n",
    "* **Acción** - Estas son las operaciones que se aplican en el RDD, que instruye a Spark para realizar el cálculo y enviar el resultado al controlador.\n",
    "\n",
    "## Ejemplo de uno vacío\n",
    "\n",
    "Crear un RDD vacío y también se puede utilizar parallelize() para crearlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDDVacio = sc.emptyRDD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostrar que esta vacio el RDDVacio\n",
    "RDDVacio.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDDVacio2 = sc.parallelize([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostrar que esta vacio el RDDVacio2\n",
    "RDDVacio2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo con datos\n",
    "\n",
    "Utilizar sparkContext.parallelize() para crear un RDD a partir de una lista o colección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un RDD con 6 datos de tipo entero\n",
    "rdd = sc.parallelize([2,4,6,8,10,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10, 12]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([[\"Hola\", 2, 3, \"Quiero\", \"Vacaciones\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hola', 2, 3, 'Quiero', 'Vacaciones']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particiones\n",
    "\n",
    "Spark particiona automáticamente los RDDs y distribuye las particiones entre diferentes nodos. Una partición en Spark es un trozo atómico de datos (división lógica de los datos) almacenado en un nodo del clúster. Las particiones son unidades básicas de paralelismo en Apache Spark. Los RDDs en Apache Spark son una colección de particiones.\n",
    "\n",
    "### Caracteristicas\n",
    "\n",
    "* Cada máquina en un cluster de spark contiene una o más particiones.\n",
    "* El número de particiones en spark es configurable y tener muy pocas o demasiadas particiones no es bueno.\n",
    "* Las particiones en Spark no abarcan varias máquinas.\n",
    "\n",
    "### Funcionalidad del particionamiento\n",
    "\n",
    "Una forma importante de aumentar el paralelismo del procesamiento de Spark es aumentar el número de ejecutores en el clúster. Sin embargo, saber cómo deben distribuirse los datos para que el clúster pueda procesarlos de forma eficiente es extremadamente importante. El secreto para conseguirlo es el particionamiento en Spark. Apache Spark gestiona los datos a través de RDDs utilizando particiones que ayudan a paralelizar el procesamiento de datos distribuidos con un tráfico de red insignificante para el envío de datos entre ejecutores. Por defecto, Apache Spark lee los datos en un RDD desde los nodos que están cerca de él.\n",
    "\n",
    "El particionamiento es un concepto importante en apache spark ya que determina cómo se accede a todos los recursos de hardware cuando se ejecuta cualquier trabajo. En apache spark, por defecto se crea una partición para cada partición HDFS de tamaño 64MB. Los RDDs se particionan automáticamente en spark sin intervención humana, sin embargo, a veces los programadores quieren cambiar el esquema de partición cambiando el tamaño de las particiones y el número de particiones en base a los requerimientos de la aplicación. Para el particionamiento personalizado los desarrolladores tienen que comprobar el número de ranuras en el hardware y cuántas tareas puede manejar un ejecutor para optimizar el rendimiento y lograr el paralelismo.\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crea una lista de 12 enteros con 3 particiones\n",
    "rdd = sc.parallelize(range(12), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle\n",
    "\n",
    "Spark SQL shuffle es un mecanismo para redistribuir o re-particionar los datos para que los datos se agrupen de manera diferente a través de las particiones, basado en el tamaño de sus datos puede necesitar reducir o aumentar el número de particiones de RDD/DataFrame usando la configuración spark.sql.shuffle.partitions o a través de código.\n",
    "\n",
    "Shuffling es un mecanismo que Spark utiliza para redistribuir los datos entre diferentes ejecutores e incluso entre máquinas. Spark shuffling dispara operaciones de transformación como **groupByKey(), reducebyKey()**, etc.\n",
    "\n",
    "## Caracteristicas\n",
    "\n",
    "Spark Shuffle es una operación costosa ya que implica lo siguiente\n",
    " \n",
    "* E/S de disco\n",
    "* Implica la serialización y deserialización de datos\n",
    "* E/S de red\n",
    "\n",
    "Cuando se crea un RDD, Spark no almacena necesariamente los datos de todas las claves de una partición ya que en el momento de la creación no podemos establecer la clave del conjunto de datos.\n",
    "\n",
    "## groupByKey\n",
    "\n",
    "La función groupByKey de Spark RDD recoge los valores de cada clave en forma de iterador\n",
    "\n",
    "Como su nombre indica la función groupByKey en Apache Spark sólo agrupa todos los valores con respecto a una única clave. A diferencia de reduceByKey no realiza ninguna operación sobre el resultado final. Simplemente agrupa los datos y los devuelve en forma de iterador. Es una operación de transformación lo que significa que su evaluación es perezosa.\n",
    "\n",
    "Ahora bien, debido a que en el RDD fuente pueden existir múltiples claves en cualquier partición, esta función requiere hacer un shuffle en todos los datos con una misma clave a una sola partición a menos que su RDD fuente ya esté particionado por clave. Y este barajado hace que esta transformación sea una transformación más amplia.\n",
    "\n",
    "Es ligeramente diferente a la transformación groupBy() ya que requiere un par clave-valor mientras que en groupBy() puede o no tener claves en el RDD fuente. La función de transformación groupBy() también necesita una función para formar una clave que no es necesaria en el caso de la función spark groupByKey().\n",
    "\n",
    "#### Puntos importantes\n",
    "\n",
    "* Apache spark groupByKey es una operación de transformación por lo que su evaluación es perezosa\n",
    "* Es una operación amplia ya que shufflea los datos de múltiples particiones y crea otro RDD\n",
    "* Esta operación es costosa ya que no utiliza el combinador local de una partición para reducir la transferencia de datos\n",
    "* No se recomienda su uso cuando se necesita hacer una agregación posterior de los datos agrupados\n",
    "* groupByKey siempre da como resultado RDDs con particiones Hash\n",
    "\n",
    "### Ejemplo de groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo basico de groupByKey\n",
    "x = sc.parallelize([\n",
    "    (\"EEUU\", 1), (\"EEUU\", 2), (\"Mexico\", 1),\n",
    "    (\"Ecuador\", 1), (\"Mexico\", 4), (\"Mexico\", 9),\n",
    "    (\"EEUU\", 8), (\"EEUU\", 3), (\"Mexico\", 4),\n",
    "    (\"Ecuador\", 6), (\"Ecuador\", 9), (\"Ecuador\", 5)], 3)\n",
    " \n",
    "# groupByKey con particiones por defecto\n",
    "y = x.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida:  3\n"
     ]
    }
   ],
   "source": [
    "# Checar particiones\n",
    "print('Salida: ', y.getNumPartitions()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida:  2\n"
     ]
    }
   ],
   "source": [
    "# Con particiones predefinidas\n",
    "y = x.groupByKey(2)\n",
    "print('Salida: ', y.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexico [1, 4, 9, 4]\n",
      "Ecuador [1, 6, 9, 5]\n",
      "EEUU [1, 2, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "# Imprimir salida\n",
    "for t in y.collect():\n",
    "    print(t[0], [v for v in t[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduceByKey\n",
    "\n",
    "La función reduceByKey de Spark combina los valores de cada clave utilizando una función de reducción asociativa.\n",
    "\n",
    "Básicamente, la función reduceByKey sólo funciona para RDDs que contienen pares de elementos clave y valor (es decir, RDDs que tienen una tupla o un mapa como elemento de datos). Es una operación de transformación, lo que significa que se evalúa perezosamente. Necesitamos pasar una función asociativa como parámetro, que se aplicará al RDD fuente y creará un nuevo RDD con los valores resultantes (es decir, un par clave-valor). Esta operación es una operación amplia, ya que los datos pueden ser shuffleados a través de las particiones.\n",
    "\n",
    "La función asociativa (que acepta dos argumentos y devuelve un único elemento) debe ser conmutativa y asociativa en su naturaleza matemática. Esto significa intuitivamente que esta función produce el mismo resultado cuando se aplica repetidamente sobre el mismo conjunto de datos RDD con múltiples particiones, independientemente del orden de los elementos. Además, realiza la fusión localmente usando la función de reducción y luego envía los registros a través de las particiones para preparar los resultados finales.\n",
    "\n",
    "#### Puntos importantes\n",
    "\n",
    "* reduceByKey es una operación de transformación en Spark, por lo que se evalúa perezosamente\n",
    "* Es una operación amplia ya que shufflea los datos de múltiples particiones y crea otro RDD\n",
    "* Antes de enviar los datos a través de las particiones, también fusiona los datos localmente usando la misma función asociativa para optimizar el barajado de datos\n",
    "* Sólo puede utilizarse con RDDs que contengan elementos del tipo pares de claves y valores\n",
    "* Acepta una función conmutativa y asociativa como argumento\n",
    "    * La función parámetro debe tener dos argumentos del mismo tipo de datos\n",
    "    * El tipo de retorno de la función también debe ser el mismo que el de los argumentos\n",
    "    \n",
    "### Ejemplo de reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo basico de reduceByKey\n",
    "# crear PairRDD x con pares clave-valor\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), (\"a\", 1),\n",
    "                    (\"b\", 1), (\"b\", 1), (\"b\", 1), (\"b\", 1)], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 5), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Aplicar la operación reduceByKey a x\n",
    "y = x.reduceByKey(lambda acum, n: acum + n)\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 5), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Definir la función asociativa por separado \n",
    "def sumFunc(acum, n):\n",
    "    return acum + n\n",
    " \n",
    "y = x.reduceByKey(sumFunc)\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones\n",
    "\n",
    "Las transformaciones RDD de PySpark son de evaluación perezosa y se utilizan para transformar/actualizar de un RDD a otro. Cuando se ejecuta en un RDD, el resultado es un único o varios RDD nuevos.\n",
    "\n",
    "Dado que los RDD son inmutables por naturaleza, las transformaciones siempre crean un nuevo RDD sin actualizar uno existente, por lo que una cadena de transformaciones de RDD crea un linaje de RDD.\n",
    "\n",
    "El linaje RDD también se conoce como gráfico de operadores RDD o gráfico de dependencia RDD.\n",
    "\n",
    "Algunas transformaciones basicas en Spark:\n",
    "\n",
    "* map()\n",
    "* flatMap()\n",
    "* filter()\n",
    "* sortByKey()\n",
    "\n",
    "## map()\n",
    "\n",
    "PySpark MAP es una transformación en PySpark que se aplica sobre todas y cada una de las funciones de un RDD / Data Frame en una aplicación Spark. El tipo de retorno es un nuevo RDD o marco de datos donde se aplica la función Map. Se utiliza para aplicar operaciones sobre cada elemento en una aplicación PySpark como una transformación, una actualización de la columna, etc.\n",
    "\n",
    "La operación Map es una simple transformación de Spark que toma un elemento del Data Frame / RDD y le aplica la lógica de transformación dada. Podemos definir nuestra propia lógica de transformación personalizada o la función derivada de la biblioteca y aplicarla utilizando la función map. El resultado devuelto será un nuevo RDD con el mismo número de elementos que el anterior.\n",
    "\n",
    "#### Sintaxis\n",
    "\n",
    "a.map (lambda x : x+1)\n",
    "\n",
    "### Ejemplos de map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 8, 9, 10, 12, 17]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo 1\n",
    "r = sc.parallelize([1,2,3,4,5,6,7,8,10,15])\n",
    "r2 = r.map(lambda x: x + 2)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 8, 9, 10, 12, 17]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo 2\n",
    "#Sin lambda\n",
    "def incremento_en_dos(x):\n",
    "    return x + 2\n",
    "\n",
    "r = sc.parallelize([1,2,3,4,5,6,7,8,10,15])\n",
    "r2 = r.map(incremento_en_dos)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7, 5, 3, 2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo 3\n",
    "#Usando strings\n",
    "r = sc.parallelize([\"holi\", \"maestra\", \"Lamia\", \"uwu\", \"<3\"])\n",
    "r2 = r.map(lambda x: len(x))\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter()\n",
    "\n",
    "PySpark Filter es una función en PySpark añadida para tratar los datos filtrados cuando se necesitan en un Spark Data Frame. La limpieza de datos es una tarea muy importante mientras se manejan los datos en PySpark y PYSPARK Filter viene con las funcionalidades que se pueden lograr por el mismo. PySpark Filter se aplica con el Data Frame y se utiliza para filtrar los datos en todo momento de manera que los datos necesarios se dejan para el procesamiento y el resto de los datos no se utilizan. Esto ayuda a un procesamiento más rápido de los datos ya que los datos no deseados o malos son limpiados por el uso de la operación de filtro en un marco de datos.\n",
    "\n",
    "La condición del filtro de PySpark se aplica en el marco de datos con varias condiciones que filtran los datos basados en los datos, la condición puede ser sobre una sola condición a múltiples condiciones utilizando la función SQL. Las filas se filtran desde el RDD / Data Frame y el resultado se utiliza para su posterior procesamiento.\n",
    "\n",
    "#### Sintaxis:\n",
    "\n",
    "La sintaxis de la función PySpark Filter es:\n",
    "\n",
    "**df.filter(#condicion)**\n",
    "\n",
    "\n",
    "* Condicion: La condición del filtro que queremos implementar.\n",
    "\n",
    "### Ejemplos de filter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Nombre|\n",
      "+------+\n",
      "|Leonel|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 1\n",
    "# Creacion del DataFrame\n",
    "\n",
    "a = spark.createDataFrame([\"Misael\", \"Ricardo\", \"Jannette\",\"Leonel\",\"Yessica\", \"Lamia\"], \"string\").toDF(\"Nombre\")\n",
    "\n",
    "# Mostrar el nombre con el filtro de Leonel\n",
    "a.filter(a.Nombre == \"Leonel\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 22,\n",
       " 24,\n",
       " 26,\n",
       " 28,\n",
       " 30,\n",
       " 32,\n",
       " 34,\n",
       " 36,\n",
       " 38,\n",
       " 40,\n",
       " 42,\n",
       " 44,\n",
       " 46,\n",
       " 48,\n",
       " 50,\n",
       " 52,\n",
       " 54,\n",
       " 56,\n",
       " 58,\n",
       " 60,\n",
       " 62,\n",
       " 64,\n",
       " 66,\n",
       " 68,\n",
       " 70,\n",
       " 72,\n",
       " 74,\n",
       " 76,\n",
       " 78,\n",
       " 80,\n",
       " 82,\n",
       " 84,\n",
       " 86,\n",
       " 88,\n",
       " 90,\n",
       " 92,\n",
       " 94,\n",
       " 96,\n",
       " 98,\n",
       " 100]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo 2\n",
    "#filter permite seleccionar de un RDD aquellos elementos que cumplen con una condición\n",
    "def es_par(x):\n",
    "    for i in range(1,x):\n",
    "        if x % 2 == 0:\n",
    "            return True\n",
    "    return False\n",
    "r = sc.parallelize(range(1,101))\n",
    "r2 = r.filter(es_par)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatMap()\n",
    "\n",
    "En Apache Spark, Spark flatMap es una de las operaciones de transformación. La operación Tr de la función Map se aplica a todos los elementos de RDD que significa conjuntos de datos distribuidos resistentes. Estos son inmutables y una colección de registros que son particionados y estos solo pueden ser creados por operaciones (operaciones que se aplican a través de todos los elementos del conjunto de datos) como filter y map. El desarrollador de operaciones en Map tiene la facilidad de crear su propia lógica de negocio personalizada. Map() es principalmente similar a flatMap() y puede devolver sólo 0 o 1 y o más elementos de la función map().\n",
    "\n",
    "#### Sintaxis:\n",
    "\n",
    "**RDD.flatMap(<transformation function>)**\n",
    "    \n",
    "La función de transformación del código de sintaxis anterior, para cada elemento fuente del RDD, puede devolver múltiples elementos del RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionalidad\n",
    "\n",
    "Un flatMap es una operación de transformación. Se devuelve un nuevo RDD con su aplicación en cada elemento del RDD como resultado. Esto da muchos resultados, lo que significa que podemos obtener uno, dos, cero y otros muchos elementos de las aplicaciones de la operación flatMap. La operación Map está un paso por detrás de la técnica de operación flatMap y es mayormente similar.\n",
    "\n",
    "### Ejemplos de flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '4', '6', '8', '10', '12', '14', '15', '16']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo 1 : aplanar las listas\n",
    "import csv\n",
    "\n",
    "#cadenas en formato csv\n",
    "r = sc.parallelize([\"2,4,6\", \"8,10,12\", \"14,15,16\"]) \n",
    "\n",
    "r2 = r.flatMap(lambda s: list(csv.reader([s]))[0])\n",
    "r2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Torreon',\n",
       " 'Gomez Palacio',\n",
       " 'Lerdo',\n",
       " 'Monterrey',\n",
       " 'Santa Catarina',\n",
       " 'San Pedro Garza García',\n",
       " 'Juarez',\n",
       " 'Tampico',\n",
       " 'Panuco',\n",
       " 'Saltillo',\n",
       " 'Ramos Arizpe',\n",
       " 'Arteaga']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo 2 : aplanar las listas\n",
    "r = sc.parallelize([\"Torreon,Gomez Palacio,Lerdo\", \"Monterrey,Santa Catarina,San Pedro Garza García,Juarez\", \"Tampico,Panuco\",\n",
    "                    \"Saltillo,Ramos Arizpe,Arteaga\"]) \n",
    "r2 = r.flatMap(lambda s: list(csv.reader([s]))[0])\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sortByKey()\n",
    "\n",
    "La transformación de Spark sortByKey() es una operación RDD que se utiliza para ordenar los valores de la clave por orden ascendente o descendente. La función sortByKey() opera sobre un par RDD (par clave/valor).\n",
    "\n",
    "### Funcionalidad\n",
    "\n",
    "sortByKey() es una transformación.\n",
    "\n",
    "* Devuelve un RDD ordenado por Clave.\n",
    "* La ordenación puede hacerse en (1) Ascendente O (2) Descendente O (3) personalizada\n",
    "\n",
    "Funcionarán con cualquier tipo de clave K que tenga un Ordering[K] implícito en el ámbito. Los objetos Ordering ya existen para todos los tipos primitivos estándar. Los usuarios también pueden definir sus propios ordenamientos para los tipos personalizados, o para anular el ordenamiento por defecto. Se utilizará la ordenación implícita que se encuentre en el ámbito más cercano.\n",
    "\n",
    "Cuando se llama a Dataset of (K, V) donde k es Ordered devuelve un conjunto de datos de pares (K, V) ordenados por claves en orden ascendente o descendente, según se especifique en el argumento ascendente.\n",
    "\n",
    "### Ejemplos de sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4', 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo 1\n",
    "\n",
    "rdd = sc.parallelize([('a', 1), ('c', 2), ('4', 3), ('d', 4), ('5', 5), ('7', 6)])\n",
    "\n",
    "# Muestra el elemento ordenado\n",
    "rdd.sortByKey().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', 3), ('5', 5), ('7', 6), ('a', 1), ('c', 2), ('d', 4)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra ordenados los elementos\n",
    "rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CDMX', 3), ('Coahuila', 2), ('Durango', 1), ('Sinaloa', 5), ('Veracruz', 4)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo 2\n",
    "rdd = sc.parallelize([(\"Durango\", 1), (\"Coahuila\", 2), (\"CDMX\", 3), (\"Veracruz\", 4), (\"Sinaloa\", 5)])\n",
    "\n",
    "rdd2 = rdd.sortByKey()\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Veracruz', 4), ('Sinaloa', 5), ('Durango', 1), ('Coahuila', 2), ('CDMX', 3)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ordenación inversa\n",
    "rdd2 = rdd.sortByKey(False)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acciones\n",
    "\n",
    "Las acciones RDD son operaciones que devuelven los valores brutos, es decir, cualquier función RDD que devuelva algo distinto a RDD[T] se considera una acción en spark programming.\n",
    "\n",
    "Como se mencionó en Transformaciones RDD, todas las transformaciones son perezosas, lo que significa que no se ejecutan de inmediato y las funciones de acción se activan para ejecutar las transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate()\n",
    "\n",
    "Agregue los elementos de cada partición, y luego los resultados de todas las particiones.\n",
    "\n",
    "### Funcionalidad\n",
    "\n",
    "Tres parametros: \n",
    "\n",
    "1. Un valor inicial para el acumulador, zeroValue que tendrá tipo C\n",
    "2. una función seqop para combinar elementos de nuestro RDD (de tipo T) con el acumulador de tipo C, devolviendo un valor de tipo C (C x T -> C).\n",
    "3. Una función combOp para combinar dos acumuladores de tipo C y devolver un valor de tipo C.\n",
    "\n",
    "Cuenta las h en el RDD\n",
    "\n",
    "### Ejemplo de aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([\"santos\", \"monterrey\", \"tigres\", \"tijuana\"])\n",
    "#          (int, cuenta las \"s\", suma los acumuladores)  => (valor_inicial, map, reduce)\n",
    "r.aggregate(0, lambda c, s : c + s.count(\"s\"), lambda c1, c2: c1 + c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count()\n",
    "\n",
    "La acción \"count\" contará el número de elementos del RDD.\n",
    "\n",
    "### Ejemplos de count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo 1\n",
    "r = sc.parallelize([\"santos\", \"monterrey\", \"tigres\", \"tijuana\"])\n",
    "r.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo 2\n",
    "r = sc.parallelize([1,2,3,4,5,6,7,8,10,15])\n",
    "r.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo 3\n",
    "r = sc.parallelize(range(15))\n",
    "r.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## countByValue()\n",
    "\n",
    "La acción countByValue() puede utilizarse para averiguar la aparición de cada elemento en el RDD.\n",
    "\n",
    "### Ejemplos de countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {3: 2, 'monterrey': 1, 'tigres': 1, 'tijuana': 1})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo 1\n",
    "r = sc.parallelize([3, \"monterrey\", \"tigres\", \"tijuana\", 3])\n",
    "r.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 1, 2: 2, 3: 2, 6: 1, 7: 1, 8: 1, 10: 1, 15: 2, 16: 1})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo 2\n",
    "r = sc.parallelize([1,2,2,3,3,6,7,8,10,15,15,16])\n",
    "r.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'Misael': 3, 'Lamia': 4, 'Chespi': 2, 'Oswi': 1, 'Gil': 2})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo 3\n",
    "r = sc.parallelize([\"Misael\", \"Misael\", \"Lamia\", \"Chespi\", \"Misael\", \"Lamia\", \"Lamia\", \"Chespi\", \"Oswi\", \"Gil\", \"Gil\", \"Lamia\"])\n",
    "r.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce()\n",
    "\n",
    "La función reduce de Spark RDD reduce los elementos de este RDD utilizando el operador binario conmutativo y asociativo especificado.\n",
    "\n",
    "Es una operación de acción de RDD, lo que significa que desencadenará todas las transformaciones alineadas en el RDD base (o en el DAG) que no se ejecutan y luego ejecutará la operación de acción en el último RDD. Esta operación es también una operación amplia. En el sentido de que la ejecución de esta operación resulta en la distribución de los datos a través de las múltiples particiones.\n",
    "\n",
    "Acepta una función con (que acepta dos argumentos y devuelve un solo elemento) que debe ser Conmutativa y Asociativa en la naturaleza matemática. Esto significa intuitivamente que esta función produce el mismo resultado cuando se aplica repetidamente sobre el mismo conjunto de datos RDD con múltiples particiones, independientemente del orden de los elementos.\n",
    "\n",
    "#### Puntos importantes\n",
    "\n",
    "* reduce es una operación de acción en Spark, por lo que desencadena la ejecución del DAG y se ejecuta en el RDD final\n",
    "* Es una operación amplia ya que baraja los datos de múltiples particiones y los reduce a un único valor\n",
    "* Acepta una función conmutativa y asociativa como argumento\n",
    "    * La función parámetro debe tener dos argumentos del mismo tipo de datos\n",
    "    * El tipo de retorno de la función también debe ser el mismo que el de los argumentos\n",
    "    \n",
    "### Ejemplos de reduce()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 1\n",
    "# reducir los números del 1 al 10 sumándolos\n",
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "sumAcum = x.reduce(lambda acum, n: acum + n)\n",
    "print(sumAcum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3628800\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 2\n",
    " \n",
    "# reducir los números del 1 al 10 multiplicandolos\n",
    "mulAcum = x.reduce(lambda acum, n: acum * n)\n",
    "print(mulAcum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# definiendo una función de reduce lambda\n",
    "def sumaAcumulada(acum, n):\n",
    "    return acum + n\n",
    " \n",
    "sumAcum = x.reduce(sumaAcumulada)\n",
    "print(sumAcum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast variable\n",
    "\n",
    "En PySpark RDD y DataFrame, las variables de broadcast son variables compartidas de sólo lectura que se almacenan en caché y están disponibles en todos los nodos de un clúster para que las tareas puedan acceder a ellas o utilizarlas. En lugar de enviar estos datos junto con cada tarea, PySpark distribuye las variables de broadcast a los trabajadores utilizando algoritmos de difusión eficientes para reducir los costes de comunicación.\n",
    "\n",
    "## Caso de uso\n",
    "\n",
    "Permítanme explicar con un ejemplo cuándo utilizar las variables de difusión: supongamos que se obtiene un código de país de dos letras en un archivo y se desea transformarlo en el nombre completo del estado (por ejemplo, DGO en Durango, COAH en Coahuila, etc.) mediante una búsqueda en el mapeo de referencia. En algunos casos, estos datos pueden ser de gran tamaño y es posible que haya muchas búsquedas de este tipo (como el código postal, etc.).\n",
    "\n",
    "En lugar de distribuir esta información junto con cada tarea a través de la red (lo que supone una sobrecarga y una pérdida de tiempo), podemos utilizar la variable de difusión para almacenar en caché esta información de búsqueda en cada máquina y las tareas utilizan esta información en caché mientras ejecutan las transformaciones.\n",
    "\n",
    "## ¿Cómo funciona PySpark Broadcast?\n",
    "\n",
    "Las variables Broadcast se utilizan de la misma manera para RDD, DataFrame.\n",
    "\n",
    "Cuando se ejecuta una aplicación PySpark RDD, DataFrame que tiene las variables Broadcast definidas y utilizadas, PySpark hace lo siguiente.\n",
    "\n",
    "* PySpark rompe el trabajo en etapas que han distribuido el  shuffling y las acciones se ejecutan con en la etapa.\n",
    "* Las etapas posteriores también se dividen en tareas\n",
    "* Spark difunde los datos comunes (reutilizables) que necesitan las tareas dentro de cada etapa.\n",
    "* Los datos difundidos se almacenan en caché en formato serializado y se deserializan antes de ejecutar cada tarea.\n",
    "\n",
    "Deberías crear y utilizar variables de broadcast para los datos que se comparten entre múltiples etapas y tareas.\n",
    "\n",
    "Tenga en cuenta que las variables de broadcast no se envían a los ejecutores con la llamada sc.broadcast(variable), sino que se enviarán a los ejecutores cuando se utilicen por primera vez.\n",
    "\n",
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Misael', 'Adame', 'Mexico', 'Durango'), ('Lamia', 'Hamdan', 'Mexico', 'Coahuila'), ('Leonel', 'Adame', 'Mexico', 'Nuevo Leon'), ('Daniel', 'Saldivar', 'Mexico', 'Coahuila')]\n"
     ]
    }
   ],
   "source": [
    "estados = {\"DGO\":\"Durango\", \"COAH\":\"Coahuila\", \"NL\":\"Nuevo Leon\"}\n",
    "broadcastEstados = sc.broadcast(estados)\n",
    "\n",
    "data = [(\"Misael\",\"Adame\",\"Mexico\",\"DGO\"),\n",
    "    (\"Lamia\",\"Hamdan\",\"Mexico\",\"COAH\"),\n",
    "    (\"Leonel\",\"Adame\",\"Mexico\",\"NL\"),\n",
    "    (\"Daniel\",\"Saldivar\",\"Mexico\",\"COAH\")\n",
    "  ]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "def convertir_estado(codigo):\n",
    "    return broadcastEstados.value[codigo]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],convertir_estado(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertir un RDD a un DataFrame\n",
    "\n",
    "En PySpark, la función toDF() del RDD se utiliza para convertir el RDD en DataFrame. Necesitaríamos convertir RDD a DataFrame ya que DataFrame proporciona más ventajas sobre RDD. Por ejemplo, DataFrame es una colección distribuida de datos organizados en columnas con nombre, similar a las tablas de una base de datos, y proporciona mejoras de optimización y rendimiento.\n",
    "\n",
    "## 1. Crear un RDD de PySpark\n",
    "\n",
    "En primer lugar, crear un RDD pasando el objeto lista de Python a la función sparkContext.parallelize(). Necesitaremos este objeto rdd para todos nuestros ejemplos a continuación.\n",
    "\n",
    "En PySpark, cuando tienes datos en una lista, lo que significa que tienes una colección de datos en la memoria del controlador de PySpark, cuando creas un RDD, esta colección va a ser paralelizada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrera = [(\"Sistemas\",1),(\"Mecatronica\",2),(\"Industrial\",3),(\"Electronica\",4),(\"Mecanica\",5),(\"Electrica\",6)]\n",
    "rdd = sc.parallelize(carrera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convertir PySpark RDD a DataFrame\n",
    "\n",
    "La conversión de PySpark RDD a DataFrame puede hacerse utilizando toDF(), createDataFrame()\n",
    "\n",
    "### 2.1 Uso de la función rdd.toDF()\n",
    "\n",
    "PySpark proporciona la función toDF() en RDD que puede ser utilizada para convertir RDD en Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+-----------+---+\n",
      "|_1         |_2 |\n",
      "+-----------+---+\n",
      "|Sistemas   |1  |\n",
      "|Mecatronica|2  |\n",
      "|Industrial |3  |\n",
      "|Electronica|4  |\n",
      "|Mecanica   |5  |\n",
      "|Electrica  |6  |\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "#Por defecto, la función toDF() crea nombres de columnas como \"_1\" y \"_2\". Este fragmento produce el siguiente esquema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toDF() tiene otra firma que toma argumentos para definir los nombres de las columnas como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carrera_nombre: string (nullable = true)\n",
      " |-- carrera_id: long (nullable = true)\n",
      "\n",
      "+--------------+----------+\n",
      "|carrera_nombre|carrera_id|\n",
      "+--------------+----------+\n",
      "|Sistemas      |1         |\n",
      "|Mecatronica   |2         |\n",
      "|Industrial    |3         |\n",
      "|Electronica   |4         |\n",
      "|Mecanica      |5         |\n",
      "|Electrica     |6         |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "carreraColumnas = [\"carrera_nombre\",\"carrera_id\"]\n",
    "df2 = rdd.toDF(carreraColumnas)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#Salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Uso de la función createDataFrame() de PySpark\n",
    "\n",
    "La clase SparkSession proporciona el método createDataFrame() para crear el DataFrame y toma el objeto rdd como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carrera_nombre: string (nullable = true)\n",
      " |-- carrera_id: long (nullable = true)\n",
      "\n",
      "+--------------+----------+\n",
      "|carrera_nombre|carrera_id|\n",
      "+--------------+----------+\n",
      "|Sistemas      |1         |\n",
      "|Mecatronica   |2         |\n",
      "|Industrial    |3         |\n",
      "|Electronica   |4         |\n",
      "|Mecanica      |5         |\n",
      "|Electrica     |6         |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "carreraDF = spark.createDataFrame(rdd, schema = carreraColumnas)\n",
    "carreraDF.printSchema()\n",
    "carreraDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es el mismo que el anterior\n",
    "\n",
    "### 2.3 Uso de createDataFrame() con el esquema StructType\n",
    "\n",
    "Cuando se infiere el esquema, por defecto el tipo de datos de las columnas se deriva de los datos y se establece nullable a true para todas las columnas. Podemos cambiar este comportamiento suministrando el esquema usando StructType - donde podemos especificar un nombre de columna, tipo de datos y nullable para cada campo/columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carrera_nombre: string (nullable = true)\n",
      " |-- carrera_id: string (nullable = true)\n",
      "\n",
      "+--------------+----------+\n",
      "|carrera_nombre|carrera_id|\n",
      "+--------------+----------+\n",
      "|Sistemas      |1         |\n",
      "|Mecatronica   |2         |\n",
      "|Industrial    |3         |\n",
      "|Electronica   |4         |\n",
      "|Mecanica      |5         |\n",
      "|Electrica     |6         |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "carreraSchema = StructType([       \n",
    "    StructField('carrera_nombre', StringType(), True),\n",
    "    StructField('carrera_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = carreraSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se obtiene el mismo resultado.\n",
    "\n",
    "# Serialización de datos con PySpark \"encoders\"\n",
    "\n",
    "La serialización se utiliza para ajustar el rendimiento en Apache Spark. Todos los datos que se envían a través de la red o se escriben en el disco o persisten en la memoria deben ser serializados. La serialización juega un papel importante en las operaciones costosas.\n",
    "\n",
    "Un codificador de un solo paso que asigna una columna de índices de categoría a una columna de vectores binarios, con un único valor por fila que indica el índice de la categoría de entrada. Por ejemplo, con 5 categorías, un valor de entrada de 2,0 se asignaría a un vector de salida de [0,0, 0,0, 1,0, 0,0]. La última categoría no se incluye por defecto (configurable a través de dropLast), porque hace que las entradas del vector sumen uno, y por tanto sean linealmente dependientes. Así, un valor de entrada de 4,0 se asigna a [0,0, 0,0, 0,0, 0,0].\n",
    "\n",
    "\n",
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+---+\n",
      "|key|alfabeto| d1| d0|\n",
      "+---+--------+---+---+\n",
      "| K1|       a|  5|  x|\n",
      "| K2|       a|  5|  x|\n",
      "| K3|       b|  5|  x|\n",
      "| K4|       b| 10|  x|\n",
      "+---+--------+---+---+\n",
      "\n",
      "(3,[1],[1.0]) [0. 1. 0.]\n",
      "(3,[0],[1.0]) [1. 0. 0.]\n",
      "(3,[0],[1.0]) [1. 0. 0.]\n",
      "(3,[1],[1.0]) [0. 1. 0.]\n",
      "(4,[2],[1.0]) [0. 0. 1. 0.]\n",
      "(4,[1],[1.0]) [0. 1. 0. 0.]\n",
      "(4,[1],[1.0]) [0. 1. 0. 0.]\n",
      "(4,[2],[1.0]) [0. 0. 1. 0.]\n",
      "(5,[0],[1.0]) [1. 0. 0. 0. 0.]\n",
      "(5,[3],[1.0]) [0. 0. 0. 1. 0.]\n",
      "(5,[1],[1.0]) [0. 1. 0. 0. 0.]\n",
      "(5,[4],[1.0]) [0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, FeatureHasher\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "valores = [(\"K1\",\"a\", 5, 'x'), (\"K2\",\"a\", 5, 'x'), (\"K3\",\"b\", 5, 'x'), (\"K4\",\"b\", 10, 'x')]\n",
    "columnas = ['key', 'alfabeto', 'd1', 'd0']\n",
    "df = sqlCtx.createDataFrame(valores, columnas)\n",
    "df.show()\n",
    "\n",
    "\n",
    "for nf in [3, 4, 5]:\n",
    "    df = df.drop('key_vector')\n",
    "    encoder = FeatureHasher(numFeatures = nf, inputCols=[\"key\"], outputCol=\"key_vector\")\n",
    "    # 'FeatureHasher' object no tiene atributo 'fit'\n",
    "    df = encoder.transform(df)\n",
    "\n",
    "    #SparseVector(int size, int[] indices, double[] values) \n",
    "    temp = df.collect()\n",
    "    for i in temp:\n",
    "        print(i.key_vector, i.key_vector.toArray())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "[1]  \"Apache Spark: Un poco de historia\". Máster en Big Data Málaga – Advanced Analytics on Big Data de la Universidad de Málaga y el grupo de investigación Khaos. https://www.bigdata.uma.es/apache-spark-un-poco-de-historia/ (accedido el 16 de junio de 2021).\n",
    "\n",
    "[2] \"Apache Spark: Introducción para principiantes\". sitiobigdata.com. https://sitiobigdata.com/2019/12/24/apache-spark-introduccion-para-principiantes/ (accedido el 16 de junio de 2021)\n",
    "\n",
    "[3] \"PySpark Documentation — PySpark 3.1.2 documentation\". Apache Spark™ - Unified Analytics Engine for Big Data. https://spark.apache.org/docs/latest/api/python/ (accedido el 16 de junio de 2021).\n",
    "\n",
    "[4] \"PySpark - What is SparkSession?\" Spark by {Examples}. https://sparkbyexamples.com/pyspark/pyspark-what-is-sparksession/ (accedido el 16 de junio de 2021).\n",
    "\n",
    "[5] \"SparkContext (Spark 3.1.2 JavaDoc)\". Apache Spark™ - Unified Analytics Engine for Big Data. https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html (accedido el 16 de junio de 2021).\n",
    "\n",
    "[6] \"PySpark - SparkContext\". TutorialsPoint. https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm (accedido el 16 de junio de 2021).\n",
    "\n",
    "[7] \"What is SparkContext? Explained\". Spark by {Examples}. https://sparkbyexamples.com/spark/spark-sparkcontext/ (accedido el 16 de junio de 2021).\n",
    "\n",
    "[8] \"PySpark parallelize() - Create RDD from a list data — SparkByExamples\". Spark by {Examples}. https://sparkbyexamples.com/pyspark/pyspark-parallelize-create-rdd/ (accedido el 16 de junio de 2021).\n",
    "\n",
    "[9] \"Apache spark RDD tutorial | learn with scala examples — spark by {examples}\". Spark by {Examples}. https://sparkbyexamples.com/spark-rdd-tutorial/ (accedido el 16 de junio de 2021).\n",
    "\n",
    "[10] \"PySpark - RDD\". TutorialsPoint. https://www.tutorialspoint.com/pyspark/pyspark_rdd.htm (accedido el 16 de junio de 2021).\n",
    "\n",
    "[11] \"Spark Read Text File | RDD | DataFrame\". Spark by {Examples}. https://sparkbyexamples.com/spark/spark-read-text-file-rdd-dataframe/ (accedido el 16 de junio de 2021).\n",
    "\n",
    "[12] \"How To Read CSV File Using Python PySpark\". NBShare. https://www.nbshare.io/notebook/187478734/How-To-Read-CSV-File-Using-Python-PySpark/ (accedido el 17 de junio de 2021).\n",
    "\n",
    "[13] J. Lopez. \"Spark — 5ta nota: Manejo de archivos de texto\". Medium. https://susejzepol.medium.com/spark-5ta-nota-manejo-de-archivos-de-texto-6ba1dd4796d5 (accedido el 17 de junio de 2021).\n",
    "\n",
    "[14] \"How Data Partitioning in Spark helps achieve more parallelism?\" ProjectPro. https://www.dezyre.com/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297 (accedido el 17 de junio de 2021).\n",
    "\n",
    "[15] \"Spark SQL Shuffle Partitions\". Spark by {Examples}. https://sparkbyexamples.com/spark/spark-shuffle-partitions/ (accedido el 17 de junio de 2021).\n",
    "\n",
    "[16] Varun. \"Apache Spark groupByKey Example\". Back To Bazics. https://backtobazics.com/big-data/spark/apache-spark-groupbykey-example/ (accedido el 17 de junio de 2021).\n",
    "\n",
    "[17] Varun. \"Apache Spark reduceByKey Example\". Back To Bazics. https://backtobazics.com/big-data/spark/apache-spark-reducebykey-example/ (accedido el 17 de junio de 2021).\n",
    "\n",
    "[18] \"PySpark RDD Transformations with examples\". Spark by {Examples}. https://sparkbyexamples.com/pyspark/pyspark-rdd-transformations/ (accedido el 17 de junio de 2021).\n",
    "\n",
    "[19] \"Introduction to PySpark Map\". Educba. https://www.educba.com/pyspark-map/ (accedido el 17 de junio de 2021).\n",
    "\n",
    "[20] S. Vithal. \"Basic Spark Transformations and Actions using pysparkcom\". DWgeek. https://dwgeek.com/basic-spark-transformations-and-actions-using-pyspark.html/ (accedido el 17 de junio de 2021).\n",
    "\n",
    "[21] \"Introduction to PySpark Filter\". Educba. https://www.educba.com/pyspark-filter/ (accedido el 17 de junio de 2021).\n",
    "\n",
    "[22] P. Predamkar. \"Spark flatMap\". Educba. https://www.educba.com/spark-flatmap/ (accedido el 17 de junio de 2021).\n",
    "\n",
    "[23] \"pyspark.RDD.sortByKey — PySpark 3.1.2 documentation\". Apache Spark™ - Unified Analytics Engine for Big Data. https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortByKey.html (accedido el 18 de junio de 2021).\n",
    "\n",
    "[24] \"Spark sortByKey() with RDD Example\". Spark by {Examples}. https://sparkbyexamples.com/apache-spark-rdd/spark-sortbykey-with-rdd-example/ (accedido el 18 de junio de 2021).\n",
    "\n",
    "[25] Gitlka. \"How SortBykey operation works in Spark\". edureka! https://www.edureka.co/community/54036/how-sortbykey-operation-works-in-spark (accedido el 17 de junio de 2021).\n",
    "\n",
    "[26] \"Apache Spark Quick Start Guide\". O’Reilly Online Learning. https://www.oreilly.com/library/view/apache-spark-quick/9781789349108/7bb4e531-5094-4fa6-9504-aa58a1efecad.xhtml (accedido el 18 de junio de 2021).\n",
    "\n",
    "[27]  Varun. \"Apache Spark reduce Example\". Back To Bazics. https://backtobazics.com/big-data/spark/apache-spark-reduce-example/ (accedido el 18 de junio de 2021).\n",
    "\n",
    "[28] \"PySpark Broadcast Variables\". Spark by {Examples}. https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/ (accedido el 18 de junio de 2021).\n",
    "\n",
    "[29] \"Convert PySpark RDD to DataFrame\". Spark by {Examples}. https://sparkbyexamples.com/pyspark/convert-pyspark-rdd-to-dataframe/ (accedido el 18 de junio de 2021).\n",
    "\n",
    "[30] \"One Hot Encoding from PySpark, Pandas, Category Encoders and skLearn\". survival8. https://survival8.blogspot.com/2020/07/one-hot-encoding-from-pyspark-pandas.html (accedido el 18 de junio de 2021).\n",
    "\n",
    "[31] \"PySpark\". TutorialsPoint. https://www.tutorialspoint.com/pyspark/pyspark_serializers.htm (accedido el 18 de junio de 2021)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
