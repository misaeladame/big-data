{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "\n",
    "Sistema diseñado para procesar datos de manera distribuida sobre clusters. Spark puede procesar cantidades de datos en el orden de terabytes incluso petabytes.\n",
    "\n",
    "El concepto de Spark es imaginar un cluster como una memoria gigante, la memoria resultante de combinar las memorias de todos los clusters. Se prioriza el uso de memoria y consigue ser muy rápido el procesamiento de la inforamción, mayor que si utilizara MapReduce (Google).\n",
    "\n",
    "Spark utiliza MapReduce para algunas ateas de clasificación mediante regresión. Apache Spark esta implementado en Scala que es ejecutado en la máquina virtual de Java. Además de Spark ofrece interfaces de programación para Java, Python y R.\n",
    "\n",
    "## Caracteristicas\n",
    "\n",
    "* Velocidad de procesamiento\n",
    "* Soporte multilenguaje\n",
    "* Análisis avanzado\n",
    "\n",
    "Se puede desarrollar en tres maneras:\n",
    "1. solo : standalone. Utiliza como base HDFS y encima se encuentra spark.\n",
    "2. Hadoop (Yarn, adminsitrador de recursos) : Base HDFS -> Yarn/Mesos(kernel administrador del cluster) -> Spark\n",
    "3. Spark con MapReduce (Spark In MapReduce, SIMR) : Base HDFS -> MapReduce y dentro se encuentra Spark\n",
    "\n",
    "![apache-spark.jpg](img/apache-spark.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Componentes de Spark\n",
    "\n",
    "![apache-spark-core.jpg](img/apache-spark-core.jpg)\n",
    "\n",
    "Cuando se ejecuta Spark en un ambiente distribuida se distinguen dos tipos de procesos: driver y executor. Un proceso driver conectado a 3 procesos excutor localizados en dos nodos del cluster:\n",
    "\n",
    "![gestor-cluster.png](img/gestor-cluster.png)\n",
    "\n",
    "driver: proceso principal. Este proceso tiene un objeto SparkContext que te permite conectar con el gestor del cluster y reservar procesos executor en los distintos nodos del cluster. Cada uno de los nodos del cluster (tambien se conocen como worker) pordrá ejecutar uno o varios procesos executor que almaenará fragmentos de los datos del programa y realizará operaciones sobre ellos. Durante la ejecución irá enviando peticiones a los distintos procesos executor que pueden contactar enre ellos para realizar tareas y comunicar con el proceso driver para devolver resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD (Resilient Distributed Datasets)\n",
    "\n",
    "Tipo de datos básicos. Estos datos almacenan información de manera distribuida entre todos los equipos del cluster. Durante la ejecución de un programa Sparck se construyen varios RDDs que se dividen en distintos fragmentos y son almacenados en la memoria de los equipos del cluster.\n",
    "\n",
    "### Caracteristicas\n",
    "\n",
    "Están formados por un conjunto de registros, también llamados elementos, todos del mismo tipo. Por ejemplo, si cargamos un RDD a partir de un archivo plano se creará un RDD de cadenas de texto, una por cada linea del archivo.\n",
    "\n",
    "En Scala o Java, al declarar un RDD se debe definir el tipo de los registros:\n",
    "* Lenguajes estáticos : RDD[String] en Scala y JavaRDD en Java.\n",
    "* Lenguajes dinámicos : Python se pueden mezclar los tipos de datos.\n",
    "\n",
    "RDDs han sido diseñados desde el inicio para ser distribuidos: Los registros que lo componen se repartirán entre los clúster.\n",
    "\n",
    "Para realizar esta distribución: los RDDs se dividen en particiones. Cada partición se almacena únciamente en un proceso executor dentro de un nodo del clúster, aunque un proceso executor puede albergar distintas particiones de distintos RDDs. El número de particiones en las que dividir un RDD se puede configurar e incluso cambiar a lo largo de la ejecución, por default es el número de núcleos de procesamiento disponibles en el clúster.\n",
    "\n",
    "Para decidir qué registros forman parte de cada partición, Spark utiliza particionadores, que son funciones que toman un registro y devuelven el número de la partición a la que pertenecen. Estos particionadores se puede configurar si se desea mejorar el rendimiento o dejar el default.\n",
    "\n",
    "Los RDDs son inmutables: no se puede modificar ni actualizar. Una vez creado, así permanece hasta que se termina la ejecución del programa.\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "Cómo se pordría particionar un RDD de 13 parejas (int, str) sobre 3 procesos executor utilizando el rango de valores del primer elemento de la pareja.\n",
    "\n",
    "![particionandoRDD.png](img/particionandoRDD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones que admite los RDDs\n",
    "Transformaciones y acciones pero ninguna de ellas modifica el RDD.\n",
    "\n",
    "### Transformaciones\n",
    "Son operaciones que toman un RDD de partida y crean un nuevo RDD, dejando el original intacto.\n",
    "\n",
    "Ejemplos: son aplicar una función a todos los registros del RDD (por ejemplo, sumar una cierta cantidad), filtrar únicamente aquellos registros que cumplan una cierta condición u ordenarlos mediante algún campo.\n",
    "\n",
    "### Acciones\n",
    "\n",
    "Son operaciones que realizan algún cómputo sobre el RDD y devuelven un valor, dejando también el RDD original inalterado.\n",
    "\n",
    "Ejemplos: sumar todos los elementos almacenados en un RDD de números, generando un valor final, o \"vaciar\" un RDD a un archivo de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepto importante del RDD: Resilencia\n",
    "\n",
    "Tienen la capacidad de recuperar su estado inicial cuando exista algún problema. Esto se debe a que los RDDs son particionados y cada particion ha sido almacenada en un proceso executor. Por lo que apartir de su estado inicial, repite las transformaciones que tiene programadas. De esta manera, regenera las particiones perdidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext\n",
    "\n",
    "método inicial para trabajar con PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Prueba\") # sc objeto que apunta a SC al cluster local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prueba'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crando un RDD de 5 enteros: se crea en la memoria del proceso driver\n",
    "r = sc.parallelize([1,2,3,4,5])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creando un RDD de 3 cadenas de texto\n",
    "r = sc.parallelize([\"hola\", \"hi\", \"ciso\"])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
